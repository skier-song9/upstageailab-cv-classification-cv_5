{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from types import SimpleNamespace\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time\n",
    "from PIL import Image\n",
    "from zoneinfo import ZoneInfo\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c173d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. YAML파일을 읽어 설정 반환\n",
    "def load_config(config_path='./config.yaml'):\n",
    "    with open(config_path, 'r') as file:\n",
    "        cfg = yaml.safe_load(file)\n",
    "    return SimpleNamespace(**cfg)\n",
    "\n",
    "# 2. 랜덤성 제어\n",
    "def set_seed(seed: int = 256):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 3. ImageDataset Class (Baseline 코드 동일)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df:pd.DataFrame, path, transform=None):\n",
    "        self.df = df\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df.iloc[idx]\n",
    "        # img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        img = Image.open(os.path.join(self.path, name))\n",
    "        if self.transform:\n",
    "            # img = self.transform(image=img)['image']\n",
    "            img = self.transform(image=np.array(img))['image']\n",
    "        return img, target\n",
    "    \n",
    "# 4. EarlyStopping Class (as requested)\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=1e-6, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model_state_dict = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            #현재의 모델로 self.best_loss, self.best_model_state_dict 업데이트\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "\t\t\t#val_loss가 best_loss보다 좋을 때 > self.best_loss와 self.best_model 업데이트\n",
    "            self.best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        else:\n",
    "\t\t\t#val_loss가 더 안 좋을 때 > patience 증가하고 early stop 여부 확인\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                if self.restore_best_weights and self.best_model_state_dict is not None:\n",
    "                    model.load_state_dict(self.best_model_state_dict)\n",
    "                return True # ealy stopped\n",
    "        return False # end with no early stop\n",
    "    \n",
    "    def restore_best(self, model):\n",
    "        if self.best_loss is not None and self.best_model_state_dict is not None:\n",
    "            print(f\"Restore model_state_dict of which best_loss: {self.best_loss:.6f}\")\n",
    "            model.load_state_dict(self.best_model_state_dict)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "# 5. Augmentation\n",
    "def get_augmentation(cfg):\n",
    "    common_resize_transform = A.Compose([\n",
    "        # 긴 변을 기준으로 종횡비를 유지하며 resize\n",
    "        A.LongestMaxSize(max_size=cfg.image_size),\n",
    "        # cfg.image_size 정사각형으로 만들고, 여백은 흰색으로 채움.\n",
    "        A.PadIfNeeded(min_height=cfg.image_size, min_width=cfg.image_size, border_mode=cv2.BORDER_CONSTANT, fill=(255, 255, 255), p=1.0),\n",
    "        A.Normalize(mean=cfg.norm_mean, std=cfg.norm_std),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    AUG = {\n",
    "        'eda': A.Compose([\n",
    "            # 공간 변형에 대한 증강\n",
    "            A.ShiftScaleRotate(shift_limit=(-0.05,0.05), scale_limit=(-0.15, 0.15), rotate_limit=(-20, 30), fill=(255,255,255), p=0.9),\n",
    "            # x,y 좌표 반전 \n",
    "            A.Transpose(p=0.5),\n",
    "            # Blur & Noise\n",
    "            A.OneOf([\n",
    "                A.GaussianBlur(sigma_limit=(0.5, 2.5), p=1.0),\n",
    "                A.Blur(blur_limit=(3, 9), p=1.0),\n",
    "            ], p=0.3),\n",
    "            A.GaussNoise(std_range=(0.0025, 0.2), p=0.8),\n",
    "            # Brightness, Contrast, ColorJitter\n",
    "            A.ColorJitter(brightness=0.1, contrast=0.07, saturation=0.07, hue=0.07, p=0.8),\n",
    "        ]),\n",
    "        'dilation': A.Compose([\n",
    "            A.Morphological(p=1, scale=(1, 3), operation=\"dilation\"),\n",
    "            # 공간 변형에 대한 증강\n",
    "            A.ShiftScaleRotate(shift_limit=(-0.05,0.05), scale_limit=(-0.15, 0.15), rotate_limit=(-20, 30), fill=(255,255,255), p=0.9),\n",
    "            # x,y 좌표 반전 \n",
    "            A.Transpose(p=0.5),\n",
    "            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=1),\n",
    "            A.RandomBrightnessContrast(p=1),\n",
    "        ]),\n",
    "        'erosion': A.Compose([\n",
    "            A.Morphological(p=1, scale=(2, 4), operation=\"erosion\"),\n",
    "            # 공간 변형에 대한 증강\n",
    "            A.ShiftScaleRotate(shift_limit=(-0.05,0.05), scale_limit=(-0.15, 0.15), rotate_limit=(-20, 30), fill=(255,255,255), p=0.9),\n",
    "            # x,y 좌표 반전 \n",
    "            A.Transpose(p=0.5),\n",
    "            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=1),\n",
    "            A.RandomBrightnessContrast(p=1),\n",
    "        ]),\n",
    "        \n",
    "    }\n",
    "\n",
    "    train_transforms = []\n",
    "    active_augs = [AUG[aug] for aug, active in cfg.augmentation.items() if active and aug in AUG]\n",
    "\n",
    "    if cfg.online_augmentation:\n",
    "        # online augmentation 학습 : 실시간으로 증강 기법을 적용하여, 더 다양한 증강 형태의 데이터를 학습할 수 있다.\n",
    "        # 장점 : 무한한 다양성, 과적합 방지 효과 증대, 저장 공간 효율성\n",
    "        # 단점 : 전처리 과정의 증가로 학습 시간 증가, 재현성이 떨어짐. 너무 많은 증강 기법을 적용하면, 의도치 않은 결과가 나올 수 있다.\n",
    "        if active_augs:\n",
    "            online_transform = A.Compose([\n",
    "                A.OneOf(active_augs, p=0.85), # 85% 확률로 active_augs에 설정된 증강 기법들이 적용된다. 15% 확률로 원본 train 데이터를 사용한다.\n",
    "                common_resize_transform # Resize 기법은 항상 동일하게.\n",
    "            ])\n",
    "            train_transforms.append(online_transform)\n",
    "        else: # 따로 지정한 증강 기법이 없는 경우, Resize 기법만 사용\n",
    "            train_transforms.append(common_resize_transform)\n",
    "    else:\n",
    "        # offline augmentation 학습 : 개별적으로 dataset을 만들어 ConcatDataset을 최종 생성한다. 모든 증강 기법을 적용 가능하고, 마치 데이터셋 개수 자체가 늘어난 것 같은 효과를 준다. (원래라면, 증강한 데이터를 저장해야 하지만 이건 생략.)\n",
    "        # 단점 : 다양성 제한\n",
    "        if active_augs:\n",
    "            for aug_pipeline in active_augs: # 각각의 aug를 transform_func으로 만든다.\n",
    "                train_transforms.append(A.Compose([aug_pipeline, common_resize_transform]))\n",
    "        else:\n",
    "            train_transforms.append(common_resize_transform)\n",
    "\n",
    "    # validation 증강은 기본 증강만 사용한다.\n",
    "    val_transform = common_resize_transform\n",
    "\n",
    "    # Validation transform with 'eda' augmentation to simulate test conditions\n",
    "    # tta_transform은 TTA에서도 사용할 증강이다.\n",
    "    tta_transform = A.Compose([\n",
    "        AUG['eda'],\n",
    "        common_resize_transform\n",
    "    ])\n",
    "\n",
    "    return train_transforms, val_transform, tta_transform\n",
    "\n",
    "# 6. Activation, Model, Criterion\n",
    "def get_activation(activation_option):\n",
    "    ACTIVATIONS = {\n",
    "        'None': None,\n",
    "        'ReLU': nn.ReLU, # 음수는 0으로, 양수는 선형함수\n",
    "        'LeakyReLU': nn.LeakyReLU, # 음수도 일부 통과 -> Dead ReLU 방지\n",
    "        'ELU': nn.ELU, # 음수도 일부 통과 → 출력 평균이 0에 가깝도록 함으로써 학습 안정화 도움\n",
    "        'SELU': nn.SELU, # ELU에 스케일링 계수를 곱해 신경망을 자기 정규화, 특정 조건 (예: fully connected, 특정 초기화, 특정 구조)에서만 자기 정규화 효과가 잘 발휘\n",
    "        'GELU': nn.GELU, # 더 부드러운 비선형성, Transformer, BERT류\n",
    "        'Tanh': nn.Tanh, # 완만한 sigmoid\n",
    "        'PReLU': nn.PReLU, # 음수도 일부 통과 -> Dead ReLU 방지, 기울기를 학습함.\n",
    "        'SiLU': nn.SiLU, # 더 부드러운 비선형성, EfficientNet, Swin Transformer 등\n",
    "    }\n",
    "    return ACTIVATIONS[activation_option]\n",
    "\n",
    "class TimmWrapper(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name=cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            num_classes=0, global_pool='avg',\n",
    "            act_layer=get_activation(cfg.timm['activation'])\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=cfg.custom_layer['drop'])\n",
    "        self.activation = get_activation(cfg.custom_layer['activation'])()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.backbone.num_features, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            self.activation,\n",
    "            self.dropout,\n",
    "            nn.Linear(1024, 17)\n",
    "        )\n",
    "        def weight_init(m):\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                if cfg.timm['activation'] in ['Tanh']:\n",
    "                    # Xavier 초기화\n",
    "                    init.xavier_uniform_(m.weight)\n",
    "                else:\n",
    "                    # He 초기화\n",
    "                    init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        if cfg.fine_tuning == 'head':\n",
    "            # backbone 파라미터를 freeze\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            # classifier는 가중치 초기화\n",
    "            self.classifier.apply(weight_init)\n",
    "        elif cfg.fine_tuning == 'custom':\n",
    "            # 직접 커스터마이징\n",
    "            pass\n",
    "        elif cfg.fine_tuning == 'scratch':\n",
    "            self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def get_timm_model(cfg):\n",
    "    if hasattr(cfg, 'custom_layer') and cfg.custom_layer:\n",
    "        return TimmWrapper(cfg).to(cfg.device)\n",
    "\n",
    "    else: # timm 모델 구조 사용\n",
    "        model = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            num_classes=17,\n",
    "            act_layer=get_activation(cfg.timm['activation'])\n",
    "        )\n",
    "        # 모델에 따라 'head'가 있는 것도 있고 없는 것도 있다.\n",
    "        # 'head'가 있다면 dropout 비율을 설정할 수 있다.\n",
    "        if cfg.timm.get('head', False):\n",
    "            if cfg.timm['head'].get('drop'):\n",
    "                model.head.drop.p = cfg.timm['head']['drop'] # classifier head의 dropout 설정\n",
    "        return model.to(cfg.device)\n",
    "\n",
    "def get_criterion(cfg):\n",
    "    CRITERIONS = {\n",
    "        \"CrossEntropyLoss\" : nn.CrossEntropyLoss()\n",
    "    }\n",
    "    return CRITERIONS[cfg.criterion]\n",
    "\n",
    "def get_optimizer(model, cfg):\n",
    "    # SGD, RMSprop, Momentum, NAG, Adam, AdamW, NAdam, RAdam, Adafactor\n",
    "    # optimizer_params = {k: v for k, v in vars(cfg.optimizer_params).items()}\n",
    "    OPTIMIZERS = {\n",
    "        'SGD': optim.SGD(model.parameters(), lr=cfg.lr),\n",
    "        'RMSprop': optim.RMSprop(model.parameters(), lr=cfg.lr, alpha=0.99, weight_decay=cfg.weight_decay),\n",
    "        'Momentum': optim.SGD(model.parameters(), lr=cfg.lr, momentum=0.9),\n",
    "        'NAG' : optim.SGD(model.parameters(), lr=cfg.lr, momentum=0.9, nesterov=True),\n",
    "        'Adam' : optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay),\n",
    "        'AdamW': optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay),\n",
    "        'NAdam': optim.NAdam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, momentum_decay=4e-3),\n",
    "        'RAdam': optim.RAdam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay),\n",
    "        'Adafactor': optim.Adafactor(model.parameters(), lr=cfg.lr, beta2_decay=-0.8, d=1.0, weight_decay=cfg.weight_decay, maximize=False)\n",
    "    }\n",
    "    return OPTIMIZERS[cfg.optimizer_name]\n",
    "\n",
    "def get_scheduler(optimizer, cfg, steps_per_epoch):\n",
    "    # scheduler_params = {k: v for k, v in vars(cfg.scheduler_params).items()}\n",
    "    # if cfg.scheduler_name == 'OneCycleLR':\n",
    "    #     scheduler_params['steps_per_epoch'] = steps_per_epoch\n",
    "    #     scheduler_params['epochs'] = cfg.epochs\n",
    "    \n",
    "    # StepLR, ExponentialLR, CosineAnnealingLR, OneCycleLR, ReduceLROnPlateau\n",
    "    SCHEDULERS = {\n",
    "        'StepLR': lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1),\n",
    "        'ExponentialLR': lr_scheduler.ExponentialLR(optimizer, gamma=0.1),\n",
    "        'CosineAnnealingLR': lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs, eta_min=0),\n",
    "        'OneCycleLR': lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=steps_per_epoch, epochs=cfg.epochs),\n",
    "        'ReduceLROnPlateau': lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=cfg.patience-5, min_lr=0),\n",
    "    }\n",
    "    return SCHEDULERS[cfg.scheduler_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ed1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModule():\n",
    "\tdef __init__(self, model: torch.nn.Module, criterion, optimizer, scheduler, train_loader, valid_loader, cfg: SimpleNamespace, verbose:int =50, run=None):\n",
    "\t\t'''\n",
    "\t\tmodel, criterion, scheduler, train_loader, valid_loader 미리 정의해서 전달\n",
    "\t\tcfg : es_patience, epochs 등에 대한 hyperparameters를 namespace 객체로 입력\n",
    "\t\t'''\n",
    "\t\trequired_attrs = ['scheduler_name','patience', 'epochs']\n",
    "\t\tfor attr in required_attrs:\n",
    "\t\t\tassert hasattr(cfg, attr), f\"AttributeError: There's no '{attr}' attribute in cfg.\"\n",
    "\t\tassert verbose > 1 and verbose < cfg.epochs, f\"Logging frequency({verbose}) MUST BE smaller than EPOCHS({cfg.epochs}) and positive value.\"\n",
    "\t\t\n",
    "\t\tself.model = model\n",
    "\t\tself.criterion = criterion\n",
    "\t\tself.optimizer = optimizer\n",
    "\t\tself.scheduler = scheduler\n",
    "\t\tself.train_loader = train_loader\n",
    "\t\tself.valid_loader = valid_loader\n",
    "\t\tself.cfg = cfg\n",
    "\t\tif getattr(cfg, \"device\", False):\n",
    "\t\t\tself.model.to(self.cfg.device)\n",
    "\t\telse:\n",
    "\t\t\tself.cfg.device = 'cpu'\n",
    "\t\tself.es = EarlyStopping(patience=self.cfg.patience)\n",
    "\t\t### list for plot\n",
    "\t\tself.train_losses_for_plot, self.val_losses_for_plot = [], []\n",
    "\t\tself.train_acc_for_plot, self.val_acc_for_plot = [], [] # classification\n",
    "\t\tself.train_f1_for_plot, self.val_f1_for_plot = [], [] # classification\n",
    "\t\t# logging frequency\n",
    "\t\tself.verbose = verbose\n",
    "\t\t# wandb run object\n",
    "\t\tself.run = run\n",
    "\t\t# Mixed Precision > 'cuda' device 에서만 가능하다.\n",
    "\t\tself.scaler = torch.amp.GradScaler(enabled=self.cfg.mixed_precision) # 기본적으로 FP16에 최적화되어 있습니다.\n",
    "\t\t\n",
    "\tdef training_step(self):\n",
    "\t\t# set train mode\n",
    "\t\tself.model.train()\n",
    "\t\trunning_loss = 0.0\n",
    "\t\tcorrect = 0 # classification\n",
    "\t\ttotal = 0\n",
    "\t\tall_preds = []\n",
    "\t\tall_targets = []\n",
    "\t\t\n",
    "\t\tfor train_x, train_y in self.train_loader: # batch training\n",
    "\t\t\ttrain_x, train_y = train_x.to(self.cfg.device), train_y.to(self.cfg.device)\n",
    "\t\t\t\n",
    "\t\t\tself.optimizer.zero_grad() # 이전 gradient 초기화\n",
    "\n",
    "\t\t\t# if self.cfg.mixed_precision: \n",
    "\t\t\t\t# autocast 컨텍스트 매니저 사용 > # FP16을 사용해 메모리 사용량 감소\n",
    "\t\t\twith torch.amp.autocast(device_type='cuda', enabled=self.cfg.mixed_precision):\n",
    "\t\t\t\toutputs = self.model(train_x)\n",
    "\t\t\t\tloss = self.criterion(outputs, train_y)\n",
    "\t\t\tself.scaler.scale(loss).backward()\n",
    "\t\t\tself.scaler.step(self.optimizer)\n",
    "\t\t\tself.scaler.update() # 다음 반복을 위해 스케일 팩터를 업데이트\n",
    "\n",
    "\t\t\t# else:\n",
    "\t\t\t# \toutputs = self.model(train_x)\n",
    "\t\t\t# \tloss = self.criterion(outputs, train_y)\n",
    "\t\t\t# \tloss.backward() # backward pass\n",
    "\t\t\t# \tself.optimizer.step() # 가중치 업데이트\n",
    "\n",
    "\t\t\tif self.cfg.scheduler_name == \"OneCycleLR\":\n",
    "\t\t\t\tself.scheduler.step()\n",
    "\t\t\t\n",
    "\t\t\trunning_loss += loss.item() * train_y.size(0) # train_loss \n",
    "\t\t\t_, predicted = torch.max(outputs, 1) # 가장 확률 높은 클래스 예측 # classification\n",
    "\t\t\tcorrect += (predicted == train_y).sum().item() # classification\n",
    "\t\t\ttotal += train_y.size(0) \n",
    "\n",
    "\t\t\tall_preds.extend(predicted.cpu().numpy())\n",
    "\t\t\tall_targets.extend(train_y.cpu().numpy())\n",
    "\n",
    "\t\t\t# **********************************************\n",
    "\t\t\t# VRAM 부족 시: 각 배치 처리 후 GPU 캐시 비우기\n",
    "\t\t\tdel train_x, train_y, outputs, loss # 사용된 변수 명시적 삭제\n",
    "\t\t\ttorch.cuda.empty_cache()           # <-- 여기에 추가\n",
    "\t\t\t# **********************************************\n",
    "\t\t\t\n",
    "\t\tepoch_loss = running_loss / total # average loss of 1 epoch\n",
    "\t\tepoch_acc = 100 * correct / total # classification\n",
    "\t\tepoch_f1 = f1_score(all_targets, all_preds, average='macro') # classification\n",
    "\t\treturn epoch_loss, epoch_acc, epoch_f1  # classification\t\t\n",
    "\t\n",
    "\tdef validation_step(self):\n",
    "\t\tself.model.eval()  # 평가 모드\n",
    "\t\tval_loss = 0\n",
    "\t\tcorrect = 0 # classification\n",
    "\t\ttotal = 0\n",
    "\t\tall_preds = []\n",
    "\t\tall_targets = []\n",
    "\t\t\n",
    "\t\twith torch.no_grad():  # gradient 계산 비활성화\n",
    "\t\t\tfor val_x, val_y in self.valid_loader: # batch training\n",
    "\t\t\t\tval_x, val_y = val_x.to(self.cfg.device), val_y.to(self.cfg.device)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# if self.cfg.mixed_precision: # FP16을 사용해 메모리 사용량 감소\n",
    "\t\t\t\t# autocast 컨텍스트 매니저 사용\n",
    "\t\t\t\twith torch.amp.autocast(device_type='cuda', enabled=self.cfg.mixed_precision):\n",
    "\t\t\t\t\toutputs = self.model(val_x)\n",
    "\t\t\t\t\tloss = self.criterion(outputs, val_y)\n",
    "\t\t\t\t# else:\n",
    "\t\t\t\t# \toutputs = self.model(val_x)\n",
    "\t\t\t\t# \tloss = self.criterion(outputs, val_y)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\tval_loss += loss.item() * val_y.size(0)\n",
    "\t\t\t\t_, predicted = torch.max(outputs, 1) # classification\n",
    "\t\t\t\tcorrect += (predicted == val_y).sum().item() # classification\n",
    "\t\t\t\ttotal += val_y.size(0)\n",
    "\n",
    "\t\t\t\tall_preds.extend(predicted.cpu().numpy())\n",
    "\t\t\t\tall_targets.extend(val_y.cpu().numpy())\n",
    "\n",
    "\t\t\t\t# **********************************************\n",
    "\t\t\t\t# VRAM 부족 시: 각 배치 처리 후 GPU 캐시 비우기\n",
    "\t\t\t\tdel val_x, val_y, outputs, loss # 사용된 변수 명시적 삭제\n",
    "\t\t\t\ttorch.cuda.empty_cache()           # <-- 여기에 추가\n",
    "\t\t\t\t# **********************************************\n",
    "\t\t\n",
    "\t\tepoch_loss = val_loss / total # average loss of 1 epoch\n",
    "\t\tepoch_acc = 100 * correct / total # classification\n",
    "\t\tepoch_f1 = f1_score(all_targets, all_preds, average='macro') # classification\n",
    "\t\treturn epoch_loss, epoch_acc, epoch_f1 # classification\n",
    "\t\n",
    "\tdef training_loop(self):\n",
    "\t\ttry:\n",
    "\t\t\t# reset loss list for plots\n",
    "\t\t\tself.train_losses_for_plot, self.val_losses_for_plot = [], []\n",
    "\t\t\tself.train_acc_for_plot, self.val_acc_for_plot = [], []\n",
    "\t\t\tepoch_counter = 0\n",
    "\t\t\tepoch_timer = []\n",
    "\t\t\tdone = False\n",
    "\t\t\t\n",
    "\t\t\tpbar = tqdm(total=self.cfg.epochs)\n",
    "\t\t\twhile not done and epoch_counter<self.cfg.epochs:\n",
    "\t\t\t\tst = time.time()\n",
    "\t\t\t\tepoch_counter += 1\n",
    "\t\t\t\t\n",
    "\t\t\t\t# train\n",
    "\t\t\t\t# train_loss = self.training_step() # regression\n",
    "\t\t\t\ttrain_loss, train_acc, train_f1 = self.training_step() # classification\n",
    "\t\t\t\t\n",
    "\t\t\t\tself.train_losses_for_plot.append(train_loss)\n",
    "\t\t\t\tself.train_acc_for_plot.append(train_acc) # classification\n",
    "\t\t\t\tself.train_f1_for_plot.append(train_f1) # classification\n",
    "\n",
    "\t\t\t\t# scheduler의 종류에 따라 val_loss를 전달하거나 그냥 step() 호출.\n",
    "\t\t\t\tif self.cfg.scheduler_name == \"OneCycleLR\":\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\telif self.cfg.scheduler_name == \"ReduceLROnPlateau\":\n",
    "\t\t\t\t\tself.scheduler.step(val_loss)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.scheduler.step()\n",
    "\n",
    "\t\t\t\t# validation\n",
    "\t\t\t\t# val_loss = self.validation_step() # regression\n",
    "\t\t\t\tval_loss, val_acc, val_f1 = self.validation_step()  # classification\n",
    "\t\t\t\tself.val_losses_for_plot.append(val_loss)\n",
    "\t\t\t\tself.val_acc_for_plot.append(val_acc) # classification\n",
    "\t\t\t\tself.val_f1_for_plot.append(val_f1) # classification\n",
    "\n",
    "\t\t\t\tepoch_timer.append(time.time() - st)\n",
    "\t\t\t\tpbar.update(1)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif self.run is not None:\n",
    "\t\t\t\t\t# print('wandb logging...')\n",
    "\t\t\t\t\tepoch_log = {\n",
    "\t\t\t\t\t\t'train_loss': train_loss,\n",
    "\t\t\t\t\t\t'train_accuracy': train_acc,\n",
    "\t\t\t\t\t\t'train_f1': train_f1,\n",
    "\t\t\t\t\t\t'val_loss': val_loss,\n",
    "\t\t\t\t\t\t'val_accuracy': val_acc,\n",
    "\t\t\t\t\t\t'val_f1': val_f1, \n",
    "\t\t\t\t\t\t'learning_rate': self.optimizer.param_groups[0]['lr'],\n",
    "\t\t\t\t\t}\n",
    "\n",
    "\t\t\t\t\t# logging weights & gradients\n",
    "\t\t\t\t\tall_grads = []\n",
    "\t\t\t\t\tall_weights = []\n",
    "\t\t\t\t\tfor param in self.model.parameters():\n",
    "\t\t\t\t\t\tif param.data is not None:\n",
    "\t\t\t\t\t\t\tall_weights.append(param.data.cpu().view(-1))\n",
    "\t\t\t\t\t\tif param.grad is not None:\n",
    "\t\t\t\t\t\t\tall_grads.append(param.grad.cpu().view(-1))\n",
    "\t\t\t\t\tif all_grads:\n",
    "\t\t\t\t\t\tepoch_log['weight/all'] = wandb.Histogram(torch.cat(all_weights))\n",
    "\t\t\t\t\tif all_weights:\n",
    "\t\t\t\t\t\tepoch_log['weights/all'] = wandb.Histogram(torch.cat(all_weights))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tself.run.log(epoch_log, step=epoch_counter) # wandb logging\n",
    "\t\t\t\tif epoch_counter == 1 or epoch_counter % self.verbose == 0:\n",
    "\t\t\t\t\t# self.verbose epoch마다 logging\n",
    "\t\t\t\t\tmean_time_spent = np.mean(epoch_timer)\n",
    "\t\t\t\t\tepoch_timer = [] # reset timer list\n",
    "\t\t\t\t\t# print(f\"Epoch {epoch_counter}/{self.cfg.epochs} [Time: {mean_time_spent:.2f}s], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.8f}\")\n",
    "\t\t\t\t\tprint(f\"Epoch {epoch_counter}/{self.cfg.epochs} [Time: {mean_time_spent:.2f}s], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.8f}\\n Train ACC: {train_acc:.2f}%, Validation ACC: {val_acc:.2f}%\\n Train F1: {train_f1:.4f}, Validation F1: {val_f1:.4f}\") # classification\n",
    "\n",
    "\t\t\t\tif self.es(self.model, val_loss):\n",
    "\t\t\t\t\t# early stopped 된 경우 if 문 안으로 들어온다.\n",
    "\t\t\t\t\tdone = True\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(e)\n",
    "\t\t\treturn False # training loop failed\n",
    "\t\treturn True # training loop succeed\n",
    "\t\t\n",
    "\tdef plot_loss(self, show:bool=False, savewandb:bool=True, savedir:str=None):\n",
    "\t\t\"\"\"loss, accuracy, f1-score에 대한 그래프 시각화 함수\n",
    "\n",
    "\t\t:param bool show: plt.show()를 실행할 건지, defaults to False\n",
    "\t\t:param bool savewandb: wandb logging에 plot을 시각화하여 저장할 건지, defaults to True\n",
    "\t\t:param str savedir: plot을 저장할 디렉토리를 설정, None이면 저장 안 함, defaults to None\n",
    "\t\t:return _type_: None\n",
    "\t\t\"\"\"\n",
    "\t\timport matplotlib.pyplot as plt\n",
    "\t\tfig, ax = plt.subplots(figsize=(6, 4))\n",
    "\t\tplt.plot(range(len(self.train_losses_for_plot)),self.train_losses_for_plot,color='blue',label='train_loss')\n",
    "\t\tplt.plot(range(len(self.val_losses_for_plot)),self.val_losses_for_plot,color='red',label='val_loss')\n",
    "\t\tplt.axhline(y=1e-3, color='red', linestyle='--', label='(Overfit)')\n",
    "\t\tplt.legend()\n",
    "\t\tplt.xlabel(\"Epoch\")\n",
    "\t\tplt.ylabel(\"Loss\")\n",
    "\t\tplt.title(\"Train/Validation Loss plot\")\n",
    "\t\tif savedir is not None:\n",
    "\t\t\tif os.path.exists(savedir):\n",
    "\t\t\t\tos.makedirs(savedir, exist_ok=True)\n",
    "\t\t\tsavepath = os.path.join(savedir, \"loss_plot.png\")\n",
    "\t\t\tplt.savefig(savepath)\n",
    "\t\t\tprint(f\"⚙️loss plot saved in {savepath}\")\n",
    "\t\tif show:\n",
    "\t\t\tplt.show()\n",
    "\t\tif savewandb and self.run is not None:\n",
    "\t\t\tself.run.log({'loss_plot': wandb.Image(fig)}) # wandb\n",
    "\t\tplt.clf()\n",
    "\t\t\n",
    "\t\t# classification\n",
    "\t\tfig, ax = plt.subplots(figsize=(6, 4))\n",
    "\t\tplt.plot(range(len(self.train_acc_for_plot)),self.train_acc_for_plot,color='blue',label='train_acc')\n",
    "\t\tplt.plot(range(len(self.val_acc_for_plot)),self.val_acc_for_plot,color='red',label='val_acc')\n",
    "\t\tplt.axhline(y=99.0, color='red', linestyle='--', label='(99%)')\n",
    "\t\tplt.legend()\n",
    "\t\tplt.xlabel(\"Epoch\")\n",
    "\t\tplt.ylabel(\"Accuracy(%)\")\n",
    "\t\tplt.title(\"Train/Validation Accuracy Plot\")\n",
    "\t\tplt.grid()\n",
    "\t\tif savedir is not None:\n",
    "\t\t\tsavepath = os.path.join(savedir, \"accuracy_plot.png\")\n",
    "\t\t\tplt.savefig(savepath)\n",
    "\t\t\tprint(f\"⚙️accuracy plot saved in {savepath}\")\n",
    "\t\tif show:\n",
    "\t\t\tplt.show()\n",
    "\t\tif savewandb and self.run is not None:\n",
    "\t\t\tself.run.log({'accuracy_plot': wandb.Image(fig)}) # wandb\n",
    "\t\tplt.clf()\n",
    "\n",
    "\t\t# classification\n",
    "\t\tfig, ax = plt.subplots(figsize=(6, 4))\n",
    "\t\tplt.plot(range(len(self.train_f1_for_plot)),self.train_f1_for_plot,color='blue',label='train_f1')\n",
    "\t\tplt.plot(range(len(self.val_f1_for_plot)),self.val_f1_for_plot,color='red',label='val_f1')\n",
    "\t\tplt.axhline(y=0.99, color='red', linestyle='--', label='(0.99)')\n",
    "\t\tplt.legend()\n",
    "\t\tplt.xlabel(\"Epoch\")\n",
    "\t\tplt.ylabel(\"F1-score\")\n",
    "\t\tplt.title(\"Train/Validation F1-score Plot\")\n",
    "\t\tplt.grid()\n",
    "\t\tif savedir is not None:\n",
    "\t\t\tsavepath = os.path.join(savedir, \"f1_plot.png\")\n",
    "\t\t\tplt.savefig(savepath)\n",
    "\t\t\tprint(f\"⚙️f1 plot saved in {savepath}\")\n",
    "\t\tif show:\n",
    "\t\t\tplt.show()\n",
    "\t\tif savewandb and self.run is not None:\n",
    "\t\t\tself.run.log({'f1_plot': wandb.Image(fig)}) # wandb\n",
    "\t\tplt.clf()\n",
    "\t\treturn None\n",
    "\t\t\n",
    "\tdef save_experiments(self, savepath=None):\n",
    "\t\t\"\"\"\"\"\"\n",
    "\t\tsave_dict = {\n",
    "\t\t\t'model_state_dict': self.model.state_dict(),\n",
    "\t\t\t'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "\t\t\t'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "\t\t\t'cfg': vars(self.cfg) # 나중에 로드해서 CFG = SimpleNamespace(**cfg)로 복원\n",
    "\t\t}\n",
    "\t\tif savepath is not None:\n",
    "\t\t\tdirpath = os.path.dirname(savepath)\n",
    "\t\t\tif os.path.exists(dirpath):\n",
    "\t\t\t\tos.makedirs(dirpath, exist_ok=True)\n",
    "\t\t\ttorch.save(save_dict, f=savepath)\n",
    "\t\t\treturn True\n",
    "\t\treturn False\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0bdfb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Offline augmentation\n",
    "def augment_class_imbalance(cfg, train_df):\n",
    "    # Cutout 증강 파이프라인 설정\n",
    "    cutout_transform = A.Compose([\n",
    "        # 이미지 크기 조정\n",
    "        A.CoarseDropout(\n",
    "            num_holes_range=(1, 2), # 마스킹 개수\n",
    "            hole_height_range=(int(cfg.image_size * 0.05), int(cfg.image_size * 0.1)), # 마스킹의 높이 범위\n",
    "            hole_width_range=(int(cfg.image_size * 0.05), int(cfg.image_size * 0.2)), # 마스킹의 너비 범위\n",
    "            fill=(0,0,0), # 검정색 마스킹\n",
    "            p=1.0\n",
    "        )\n",
    "    ])\n",
    "    # 증강 대상 클래스\n",
    "    augment_classes = cfg.class_imbalance['aug_class']\n",
    "    max_samples = cfg.class_imbalance['max_samples']\n",
    "\n",
    "    # 증강 이미지, 라벨, ID 리스트 초기화\n",
    "    augmented_labels = []    # 증강된 이미지 라벨\n",
    "    augmented_ids = []       # 증강된 이미지 ID\n",
    "    total_augmented = 0\n",
    "    # 증강 대상 클래스 루프\n",
    "    for cls in augment_classes:\n",
    "        print(cls, \"클래스\")\n",
    "        cls_df = train_df[train_df['target'] == cls]\n",
    "        current_count = len(cls_df)\n",
    "        print(\"현재 개수:\", current_count)\n",
    "        # 목표 샘플 수에 도달하기 위해 필요한 증강 이미지 개수 계산\n",
    "        # (current_count가 max_samples보다 많으면 0 또는 음수가 됨)\n",
    "        to_generate = max_samples - current_count\n",
    "        print(\"증강 개수:\", to_generate)\n",
    "        # 만약 현재 이미지 수가 목표치보다 많거나 같으면 증강할 필요 없으므로 다음 클래스로 넘어감\n",
    "        if to_generate <= 0:\n",
    "            continue\n",
    "        # 증강할 이미지들을 원본 데이터프레임에서 샘플링 (중복 허용: replace=True)\n",
    "        # 목표 샘플 수 (to_generate)만큼 이미지를 샘플링하며,\n",
    "        # 만약 현재 이미지 개수(current_count)가 to_generate보다 적으면 중복 선택(replace=True)\n",
    "        # 이 부분은 원래 코드의 `if/else` 조건문을 통합한 것입니다.\n",
    "        sampled_df = cls_df.sample(n=to_generate, replace=False, random_state=cfg.random_seed).reset_index(drop=True)\n",
    "        # 샘플링된 각 이미지에 대해 증강 수행 및 저장\n",
    "        for idx, row in sampled_df.iterrows():\n",
    "            img_id = row['ID']\n",
    "            img_path = os.path.join(cfg.data_dir, 'train', img_id)\n",
    "            # 이미지 로드 (OpenCV는 기본적으로 BGR로 로드)\n",
    "            img = cv2.imread(img_path)\n",
    "            # BGR 이미지를 RGB로 변환 (Albumentations는 RGB를 기대)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # 정의된 Cutout 증강 적용\n",
    "            augmented_img = cutout_transform(image=img)['image']\n",
    "\n",
    "            # 증강된 이미지의 새로운 ID 생성 (기존 ID 앞에 'aug_' 접두사 추가)\n",
    "            new_id = f\"aug_{img_id}\"\n",
    "            save_path = os.path.join(cfg.data_dir, 'train', new_id)\n",
    "\n",
    "            # 증강된 RGB 이미지를 다시 BGR로 변환하여 파일로 저장\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(augmented_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # 증강된 이미지의 라벨과 ID를 리스트에 추가\n",
    "            augmented_labels.append(cls)\n",
    "            augmented_ids.append(new_id)\n",
    "            total_augmented += 1\n",
    "        print(f\"총 {total_augmented} 개의 이미지 증강\")\n",
    "    return augmented_ids, augmented_labels\n",
    "def delete_offline_augmented_images(cfg, augmented_ids):\n",
    "    train_dir = os.path.join(cfg.data_dir, 'train')\n",
    "    _ = 0\n",
    "    for filename in augmented_ids:\n",
    "        file_path = os.path.join(train_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            _ += 1\n",
    "        else:\n",
    "            print(\"Wrong filename:\", file_path)\n",
    "    print(_,\"개 이미지 제거\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d6c3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. TTA & Inference\n",
    "def tta_predict(model, dataset, tta_transform, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for enumidx, (image, _) in enumerate(tqdm(dataset, desc=\"TTA Prediction\")): # load batch\n",
    "            # read raw images from dataset_raw\n",
    "            tta_preds = []\n",
    "            image = image.clamp(0, 255).to(torch.uint8) \n",
    "            image = image.permute(1, 2, 0).cpu().numpy() # H,W,C 로 변형\n",
    "            for _ in range(5): # 5 TTA iterations\n",
    "                augmented_image = tta_transform(image=image)['image']\n",
    "                augmented_image = augmented_image.to(device)\n",
    "                augmented_image = augmented_image.unsqueeze(0) # batch, H,W,C 로 변형\n",
    "                outputs = model(augmented_image) # inference\n",
    "                tta_preds.append(outputs.softmax(1).cpu().numpy()) # append inference result\n",
    "        \n",
    "            avg_preds = np.mean(tta_preds, axis=0) # 5 TTA 예측 결과 확률값을 평균 낸다.\n",
    "            predictions.extend(avg_preds.argmax(1))\n",
    "    return predictions\n",
    "\n",
    "def do_validation(df, model, data, transform_func, cfg, run=None, show=False, savepath=None):\n",
    "    if cfg.TTA:\n",
    "        print(\"Running TTA on validation set...\")\n",
    "        val_preds = tta_predict(model, data, transform_func, cfg.device)\n",
    "    else:\n",
    "        print(\"Running Normal Validation...\")\n",
    "        val_preds = predict(model, data, cfg.device)\n",
    "    val_targets = df['target'].values\n",
    "    val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "    # 메타데이터 로드\n",
    "    meta = pd.read_csv(os.path.join(cfg.data_dir, 'meta.csv'))\n",
    "    meta_dict = zip(meta['target'], meta['class_name'])\n",
    "    meta_dict = dict(meta_dict)\n",
    "    # meta_dict[-1] = \"None\"\n",
    "    val_targets_class = list(map(lambda x: meta_dict[x], val_targets))\n",
    "    val_preds_class = list(map(lambda x: meta_dict[x], val_preds))\n",
    "    all_classes = sorted(list(set(val_targets_class + val_preds_class)))\n",
    "    cm = confusion_matrix(val_targets_class, val_preds_class, labels=all_classes)\n",
    "    plt.figure(figsize=(10, 8), dpi=100)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=all_classes, yticklabels=all_classes)\n",
    "    plt.title(f\"Validation Confusion Matrix - F1: {val_f1:.4f}\")\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    if run:\n",
    "        run.log({\"tta_val_confusion_matrix\": wandb.Image(plt)})\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()\n",
    "    return val_preds, val_f1\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader, desc=\"Prediction\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            predictions.extend(outputs.argmax(1).cpu().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f17d7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Device : cuda\n",
      "⌚ 실험 시간: 2507032145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskiersong9\u001b[0m (\u001b[33mskiersong\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/upstageailab-cv-classification-cv_5/codes/wandb/run-20250703_214544-ftu8bvxv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skiersong/upstage-img-clf/runs/ftu8bvxv' target=\"_blank\">2507032145-resnet50.tv2_in1k-opt_AdamW-sch_CosineAnnealingLR-img224-onaug_eda_dilation-clsaug_1-TTA_1-MP_1</a></strong> to <a href='https://wandb.ai/skiersong/upstage-img-clf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/skiersong/upstage-img-clf' target=\"_blank\">https://wandb.ai/skiersong/upstage-img-clf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/skiersong/upstage-img-clf/runs/ftu8bvxv' target=\"_blank\">https://wandb.ai/skiersong/upstage-img-clf/runs/ftu8bvxv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(model_name='resnet50.tv2_in1k', pretrained=True, fine_tuning='full', criterion='CrossEntropyLoss', optimizer_name='AdamW', lr=0.0001, weight_decay=1e-05, scheduler_name='CosineAnnealingLR', random_seed=256, n_folds=0, val_split_ratio=0.15, stratify=True, image_size=224, norm_mean=[0.5, 0.5, 0.5], norm_std=[0.5, 0.5, 0.5], class_imbalance={'aug_class': [1, 13, 14], 'max_samples': 78}, online_augmentation=True, augmentation={'eda': True, 'dilation': True, 'erosion': False, 'mixup': False, 'cutmix': False}, TTA=True, mixed_precision=True, timm={'activation': 'None'}, custom_layer=None, epochs=10000, patience=20, batch_size=32, wandb={'project': 'upstage-img-clf', 'log': True}, data_dir='/data/ephemeral/home/upstageailab-cv-classification-cv_5/data', device=device(type='cuda'), submission_dir='/data/ephemeral/home/upstageailab-cv-classification-cv_5/data/submissions/2507032145-resnet50.tv2_in1k-opt_AdamW-sch_CosineAnnealingLR-img224-onaug_eda_dilation-clsaug_1-TTA_1-MP_1')\n",
      "2507032145-resnet50.tv2_in1k-opt_AdamW-sch_CosineAnnealingLR-img224-onaug_eda_dilation-clsaug_1-TTA_1-MP_1\n"
     ]
    }
   ],
   "source": [
    "### main code\n",
    "project_root = '/data/ephemeral/home/upstageailab-cv-classification-cv_5'\n",
    "cfg = load_config(\n",
    "    config_path=os.path.join(project_root,'codes','config.yaml')\n",
    ")\n",
    "set_seed(cfg.random_seed)\n",
    "device = 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(\"⚙️ Device :\",device)\n",
    "cfg.device = device\n",
    "\n",
    "CURRENT_TIME = datetime.now(ZoneInfo(\"Asia/Seoul\")).strftime(\"%y%m%d%H%M\")\n",
    "print(f\"⌚ 실험 시간: {CURRENT_TIME}\")\n",
    "\n",
    "# W&B\n",
    "next_run_name = f\"{CURRENT_TIME}-{cfg.model_name}-opt_{cfg.optimizer_name}-sch_{cfg.scheduler_name}-img{cfg.image_size}-{'on' if cfg.online_augmentation else 'off'}aug_{'_'.join([aug for aug, active in cfg.augmentation.items() if active])}-clsaug_{1 if cfg.class_imbalance else 0}-TTA_{1 if cfg.TTA else 0}-MP_{1 if cfg.mixed_precision else 0}\"\n",
    "run = None # wandb 기록할 거면 True, 아니면 False.\n",
    "if hasattr(cfg, 'wandb') and cfg.wandb['log']:\n",
    "    run = wandb.init(\n",
    "        project=cfg.wandb['project'],\n",
    "        name=next_run_name,\n",
    "        config=vars(cfg),\n",
    "    )\n",
    "\n",
    "### submission 폴더 생성\n",
    "submission_dir = os.path.join(cfg.data_dir, 'submissions', next_run_name)\n",
    "try:\n",
    "    os.makedirs(submission_dir, exist_ok=False)\n",
    "    # cfg에 추가 \n",
    "    cfg.submission_dir = submission_dir\n",
    "except:\n",
    "    raise ValueError(\"같은 이름의 submission 폴더가 있습니다.\", submission_dir)\n",
    "\n",
    "print(cfg)\n",
    "print(next_run_name)\n",
    "if hasattr(cfg, 'class_imbalance') and cfg.class_imbalance:\n",
    "    assert cfg.class_imbalance['max_samples'] <= 100, \"Max_samples must be less than 73!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb97f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 클래스\n",
      "현재 개수: 39\n",
      "증강 개수: 39\n",
      "총 39 개의 이미지 증강\n",
      "13 클래스\n",
      "현재 개수: 63\n",
      "증강 개수: 15\n",
      "총 54 개의 이미지 증강\n",
      "14 클래스\n",
      "현재 개수: 42\n",
      "증강 개수: 36\n",
      "총 90 개의 이미지 증강\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "df = pd.read_csv(os.path.join(cfg.data_dir, \"train.csv\"))\n",
    "train_df, val_df = train_test_split(df, test_size=cfg.val_split_ratio, random_state=cfg.random_seed, stratify=df['target'] if cfg.stratify else None)\n",
    "\n",
    "augmented_ids, augmented_labels = [], []\n",
    "# 클래스 불균형 해소를 위한 이미지 offline 증강\n",
    "if hasattr(cfg, 'class_imbalance') and cfg.class_imbalance:\n",
    "    augmented_ids, augmented_labels = augment_class_imbalance(cfg, train_df)\n",
    "    imb_aug_df = pd.DataFrame({\n",
    "        \"ID\": augmented_ids,\n",
    "        \"target\": augmented_labels\n",
    "    })\n",
    "    # 기존 train 데이터 프레임과 병합\n",
    "    train_df = pd.concat([train_df, imb_aug_df], ignore_index=True)\n",
    "    train_df = train_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9833c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_transforms, val_transform, tta_transform = get_augmentation(cfg)\n",
    "\n",
    "if cfg.online_augmentation:\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(cfg.data_dir, \"train\"), transform=train_transforms[0])\n",
    "else:\n",
    "    datasets = [ImageDataset(train_df, os.path.join(cfg.data_dir, \"train\"), transform=t) for t in train_transforms]\n",
    "    train_dataset = ConcatDataset(datasets)\n",
    "\n",
    "val_dataset = ImageDataset(val_df, os.path.join(cfg.data_dir, \"train\"), transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# For TTA, we need a loader with raw images\n",
    "raw_transform = A.Compose([\n",
    "    ToTensorV2()\n",
    "])\n",
    "val_dataset_raw = ImageDataset(val_df, os.path.join(cfg.data_dir, \"train\"), transform=raw_transform)\n",
    "val_loader_raw = DataLoader(val_dataset_raw, batch_size=cfg.batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19f42683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = get_timm_model(cfg)\n",
    "criterion = get_criterion(cfg)\n",
    "optimizer = get_optimizer(model, cfg)\n",
    "scheduler = get_scheduler(optimizer, cfg, steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:05<14:32:35,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000 [Time: 5.24s], Train Loss: 1.6510, Validation Loss: 0.70745743\n",
      " Train ACC: 49.37%, Validation ACC: 75.00%\n",
      " Train F1: 0.4890, Validation F1: 0.7183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/10000 [01:14<13:42:23,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/10000 [Time: 4.49s], Train Loss: 0.1369, Validation Loss: 0.22940896\n",
      " Train ACC: 94.87%, Validation ACC: 92.80%\n",
      " Train F1: 0.9485, Validation F1: 0.9301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/10000 [02:28<13:41:04,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/10000 [Time: 4.51s], Train Loss: 0.0760, Validation Loss: 0.32950440\n",
      " Train ACC: 97.26%, Validation ACC: 91.10%\n",
      " Train F1: 0.9726, Validation F1: 0.9037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 39/10000 [03:12<13:40:52,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️loss plot saved in /data/ephemeral/home/upstageailab-cv-classification-cv_5/data/submissions/2507032145-resnet50.tv2_in1k-opt_AdamW-sch_CosineAnnealingLR-img224-onaug_eda_dilation-clsaug_1-TTA_1-MP_1/loss_plot.png\n",
      "⚙️accuracy plot saved in /data/ephemeral/home/upstageailab-cv-classification-cv_5/data/submissions/2507032145-resnet50.tv2_in1k-opt_AdamW-sch_CosineAnnealingLR-img224-onaug_eda_dilation-clsaug_1-TTA_1-MP_1/accuracy_plot.png\n",
      "⚙️f1 plot saved in /data/ephemeral/home/upstageailab-cv-classification-cv_5/data/submissions/2507032145-resnet50.tv2_in1k-opt_AdamW-sch_CosineAnnealingLR-img224-onaug_eda_dilation-clsaug_1-TTA_1-MP_1/f1_plot.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "trainer = TrainModule(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=val_loader,\n",
    "    cfg=cfg,\n",
    "    verbose=15,\n",
    "    run=run\n",
    ")\n",
    "\n",
    "train_result = trainer.training_loop()\n",
    "if not train_result:\n",
    "    if run:\n",
    "        run.finish()\n",
    "    if augmented_ids:\n",
    "        ### Offline Augmentation 파일 삭제\n",
    "        delete_offline_augmented_images(cfg=cfg, augmented_ids=augmented_ids)\n",
    "    raise ValueError(\"Failed to train model...\")\n",
    "\n",
    "## 모델 저장\n",
    "trainer.save_experiments(savepath=os.path.join(cfg.submission_dir, f'{next_run_name}.pth'))\n",
    "## 학습 결과 시각화\n",
    "trainer.plot_loss(\n",
    "    show=False,\n",
    "    savewandb=cfg.wandb['log'],\n",
    "    savedir=cfg.submission_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1faea931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TTA on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Prediction: 100%|██████████| 236/236 [00:15<00:00, 15.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_preds, val_f1 = do_validation(\n",
    "    df=val_df, \n",
    "    model=trainer.model, \n",
    "    data=val_dataset_raw if cfg.TTA else val_loader, \n",
    "    transform_func=tta_transform, \n",
    "    cfg=cfg, \n",
    "    run=run, \n",
    "    show=False, \n",
    "    savepath=os.path.join(cfg.submission_dir, f\"val_confusion_matrix{'_TTA' if cfg.TTA else ''}.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb8cac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TTA on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Prediction: 100%|██████████| 3140/3140 [03:26<00:00, 15.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "test_df = pd.read_csv(os.path.join(cfg.data_dir, \"sample_submission.csv\"))\n",
    "\n",
    "if cfg.TTA:\n",
    "    test_dataset_raw = ImageDataset(test_df, os.path.join(cfg.data_dir, \"test\"), transform=raw_transform)\n",
    "    test_loader_raw = DataLoader(test_dataset_raw, batch_size=cfg.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    print(\"Running TTA on test set...\")\n",
    "    test_preds = tta_predict(model, test_dataset_raw, tta_transform, device)\n",
    "else:\n",
    "    test_dataset = ImageDataset(test_df, os.path.join(cfg.data_dir, \"test\"), transform=val_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    print(\"Running inference on test set...\")\n",
    "    test_preds = predict(model, test_loader, device)\n",
    "\n",
    "pred_df = pd.read_csv(os.path.join(cfg.data_dir, \"sample_submission.csv\"))\n",
    "pred_df['target'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "662d758a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📢Submission file saved to /data/ephemeral/home/upstageailab-cv-classification-cv_5/data/submissions/2507032145-resnet50.tv2_in1k-opt_AdamW-sch_CosineAnnealingLR-img224-onaug_eda_dilation-clsaug_1-TTA_1-MP_1/2507032145-resnet50.tv2_in1k-opt_AdamW-sch_CosineAnnealingLR-img224-onaug_eda_dilation-clsaug_1-TTA_1-MP_1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>██████████▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇▇▇▇▇▇▇██████▇████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▆▇▇▇▇▇▇▇▇▇██████▇████████████████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▆▇▆▇█▇▆▅▇▆▇▇▇▇█▆██▇█▇▇▇▆▆▇█▇▇▇█▆█▇▇▆▆</td></tr><tr><td>val_f1</td><td>▁▆▆▇▆▇▇▇▆▆▇▆▇▇█▇█▆██▇██▇▇▆▆▇█▇▇▇█▇█▇▇▆▇</td></tr><tr><td>val_loss</td><td>▆▃▂▂▃▂▂▂▃▅▂█▂▂▂▂▂▄▁▂▂▂▂▂▂▄▂▃▂▃▂▂▂▂▂▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0004</td></tr><tr><td>train_accuracy</td><td>98.03371</td></tr><tr><td>train_f1</td><td>0.98051</td></tr><tr><td>train_loss</td><td>0.06573</td></tr><tr><td>val_accuracy</td><td>90.25424</td></tr><tr><td>val_f1</td><td>0.89851</td></tr><tr><td>val_loss</td><td>0.36271</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2507032145-resnet50.tv2_in1k-opt_AdamW-sch_CosineAnnealingLR-img224-onaug_eda_dilation-clsaug_1-TTA_1-MP_1</strong> at: <a href='https://wandb.ai/skiersong/upstage-img-clf/runs/ftu8bvxv' target=\"_blank\">https://wandb.ai/skiersong/upstage-img-clf/runs/ftu8bvxv</a><br> View project at: <a href='https://wandb.ai/skiersong/upstage-img-clf' target=\"_blank\">https://wandb.ai/skiersong/upstage-img-clf</a><br>Synced 5 W&B file(s), 4 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250703_214544-ftu8bvxv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Submission\n",
    "sample_submission_df = pd.read_csv(os.path.join(cfg.data_dir, \"sample_submission.csv\"))\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all(), \"pred_df에서 test 이미지가 아닌 데이터가 존재합니다.\"\n",
    "assert set(pred_df['target']).issubset(set(range(17))), \"target 컬럼에 0~16 외의 값이 있습니다.\"\n",
    "\n",
    "submission_path = os.path.join(cfg.submission_dir, f\"{next_run_name}.csv\")\n",
    "pred_df.to_csv(submission_path, index=False)\n",
    "print(f\"📢Submission file saved to {submission_path}\")\n",
    "\n",
    "if run:\n",
    "    # Log submission artifact\n",
    "    artifact = wandb.Artifact(f'submission-{next_run_name}', type='submission')\n",
    "    artifact.add_file(submission_path)\n",
    "    run.log_artifact(artifact)\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b76c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 개 이미지 제거\n"
     ]
    }
   ],
   "source": [
    "### Offline Augmentation 파일 삭제\n",
    "delete_offline_augmented_images(cfg=cfg, augmented_ids=augmented_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a589f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model load\n",
    "### 저장된 trainer를 로드하여 추론하는 경우 아래 함수로 로드\n",
    "# def load_checkpoint_model(savepath=None):\n",
    "#     if os.path.exists(savepath):\n",
    "#         checkpoint = torch.load(savepath)\n",
    "#         cfg = SimpleNamespace(**checkpoint['cfg'])\n",
    "#         model = get_timm_model(cfg)\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         return model, cfg\n",
    "#     else:\n",
    "#         return None\n",
    "    \n",
    "# model, cfg = load_checkpoint_model(\n",
    "#     savepath=os.path.join(project_root,'models','resnetrs101.tf_in1k-opt_Adam-img224-aug_0-2506301324-0001.pth')\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
