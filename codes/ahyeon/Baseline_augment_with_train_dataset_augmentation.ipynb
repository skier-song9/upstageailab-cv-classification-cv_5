{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkH9T_86lDSS"
      },
      "source": [
        "## 1. Prepare Environments\n",
        "\n",
        "* 데이터 폴더의 경로를 설정합니다.\n",
        "* 필요한 라이브러리를 import 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 한국어 설치 apt-get install에서 Error 발생 시, 'sudo' 명령어 추가해서 root 권한으로 실행.\n",
        "# !apt-get install fonts-nanum*\n",
        "# !fc-cache -fv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# utils\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import copy\n",
        "import time\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_rows',None) # 또는 숫자 지정\n",
        "pd.set_option('display.max_columns',None)\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "fe = fm.FontEntry(\n",
        "    fname='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf', # ttf 파일 저장 경로\n",
        "    name='NanumBarunGothic' # 별칭\n",
        ")\n",
        "fm.fontManager.ttflist.insert(0,fe)\n",
        "plt.rcParams.update({'font.family' : 'NanumBarunGothic'}) # 한글 패치\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "plt.rcParams['axes.unicode_minus'] =False # 음수 부호 안 깨지게\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "import torch.nn.init as init\n",
        "\n",
        "# Image Augmentation\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from augraphy import *\n",
        "\n",
        "# train dataset augmentation\n",
        "import cv2\n",
        "import itertools\n",
        "\n",
        "### random seed 고정 함수\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"모든 랜덤 시드를 고정합니다.\n",
        "\n",
        "    :param int seed: 고정할 시드 값, defaults to 42\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    try:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # 멀티 GPU용\n",
        "        \n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    except ImportError:\n",
        "        pass  # torch가 설치되어 있지 않으면 무시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "현재 jupyter notebook의 실행 경로: /data/ephemeral/home/upstageailab-cv-classification-cv_5/codes/practice\n"
          ]
        }
      ],
      "source": [
        "print(\"현재 jupyter notebook의 실행 경로:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['resnet10t.c3_in1k',\n",
              " 'resnet14t.c3_in1k',\n",
              " 'resnet18.a1_in1k',\n",
              " 'resnet18.a2_in1k',\n",
              " 'resnet18.a3_in1k',\n",
              " 'resnet18.fb_ssl_yfcc100m_ft_in1k',\n",
              " 'resnet18.fb_swsl_ig1b_ft_in1k',\n",
              " 'resnet18.gluon_in1k',\n",
              " 'resnet18.tv_in1k',\n",
              " 'resnet18d.ra2_in1k',\n",
              " 'resnet26.bt_in1k',\n",
              " 'resnet26d.bt_in1k',\n",
              " 'resnet26t.ra2_in1k',\n",
              " 'resnet32ts.ra2_in1k',\n",
              " 'resnet33ts.ra2_in1k',\n",
              " 'resnet34.a1_in1k',\n",
              " 'resnet34.a2_in1k',\n",
              " 'resnet34.a3_in1k',\n",
              " 'resnet34.bt_in1k',\n",
              " 'resnet34.gluon_in1k',\n",
              " 'resnet34.tv_in1k',\n",
              " 'resnet34d.ra2_in1k',\n",
              " 'resnet50.a1_in1k',\n",
              " 'resnet50.a1h_in1k',\n",
              " 'resnet50.a2_in1k',\n",
              " 'resnet50.a3_in1k',\n",
              " 'resnet50.am_in1k',\n",
              " 'resnet50.b1k_in1k',\n",
              " 'resnet50.b2k_in1k',\n",
              " 'resnet50.bt_in1k',\n",
              " 'resnet50.c1_in1k',\n",
              " 'resnet50.c2_in1k',\n",
              " 'resnet50.d_in1k',\n",
              " 'resnet50.fb_ssl_yfcc100m_ft_in1k',\n",
              " 'resnet50.fb_swsl_ig1b_ft_in1k',\n",
              " 'resnet50.gluon_in1k',\n",
              " 'resnet50.ra_in1k',\n",
              " 'resnet50.ram_in1k',\n",
              " 'resnet50.tv2_in1k',\n",
              " 'resnet50.tv_in1k',\n",
              " 'resnet50_gn.a1h_in1k',\n",
              " 'resnet50c.gluon_in1k',\n",
              " 'resnet50d.a1_in1k',\n",
              " 'resnet50d.a2_in1k',\n",
              " 'resnet50d.a3_in1k',\n",
              " 'resnet50d.gluon_in1k',\n",
              " 'resnet50d.ra2_in1k',\n",
              " 'resnet50s.gluon_in1k',\n",
              " 'resnet51q.ra2_in1k',\n",
              " 'resnet61q.ra2_in1k',\n",
              " 'resnet101.a1_in1k',\n",
              " 'resnet101.a1h_in1k',\n",
              " 'resnet101.a2_in1k',\n",
              " 'resnet101.a3_in1k',\n",
              " 'resnet101.gluon_in1k',\n",
              " 'resnet101.tv2_in1k',\n",
              " 'resnet101.tv_in1k',\n",
              " 'resnet101c.gluon_in1k',\n",
              " 'resnet101d.gluon_in1k',\n",
              " 'resnet101d.ra2_in1k',\n",
              " 'resnet101s.gluon_in1k',\n",
              " 'resnet152.a1_in1k',\n",
              " 'resnet152.a1h_in1k',\n",
              " 'resnet152.a2_in1k',\n",
              " 'resnet152.a3_in1k',\n",
              " 'resnet152.gluon_in1k',\n",
              " 'resnet152.tv2_in1k',\n",
              " 'resnet152.tv_in1k',\n",
              " 'resnet152c.gluon_in1k',\n",
              " 'resnet152d.gluon_in1k',\n",
              " 'resnet152d.ra2_in1k',\n",
              " 'resnet152s.gluon_in1k',\n",
              " 'resnet200d.ra2_in1k',\n",
              " 'resnetaa50.a1h_in1k',\n",
              " 'resnetaa50d.d_in12k',\n",
              " 'resnetaa50d.sw_in12k',\n",
              " 'resnetaa50d.sw_in12k_ft_in1k',\n",
              " 'resnetaa101d.sw_in12k',\n",
              " 'resnetaa101d.sw_in12k_ft_in1k',\n",
              " 'resnetblur50.bt_in1k',\n",
              " 'resnetrs50.tf_in1k',\n",
              " 'resnetrs101.tf_in1k',\n",
              " 'resnetrs152.tf_in1k',\n",
              " 'resnetrs200.tf_in1k',\n",
              " 'resnetrs270.tf_in1k',\n",
              " 'resnetrs350.tf_in1k',\n",
              " 'resnetrs420.tf_in1k',\n",
              " 'resnetv2_50.a1h_in1k',\n",
              " 'resnetv2_50d_evos.ah_in1k',\n",
              " 'resnetv2_50d_gn.ah_in1k',\n",
              " 'resnetv2_50x1_bit.goog_distilled_in1k',\n",
              " 'resnetv2_50x1_bit.goog_in21k',\n",
              " 'resnetv2_50x1_bit.goog_in21k_ft_in1k',\n",
              " 'resnetv2_50x3_bit.goog_in21k',\n",
              " 'resnetv2_50x3_bit.goog_in21k_ft_in1k',\n",
              " 'resnetv2_101.a1h_in1k',\n",
              " 'resnetv2_101x1_bit.goog_in21k',\n",
              " 'resnetv2_101x1_bit.goog_in21k_ft_in1k',\n",
              " 'resnetv2_101x3_bit.goog_in21k',\n",
              " 'resnetv2_101x3_bit.goog_in21k_ft_in1k',\n",
              " 'resnetv2_152x2_bit.goog_in21k',\n",
              " 'resnetv2_152x2_bit.goog_in21k_ft_in1k',\n",
              " 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k',\n",
              " 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384',\n",
              " 'resnetv2_152x4_bit.goog_in21k',\n",
              " 'resnetv2_152x4_bit.goog_in21k_ft_in1k']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "timm.list_models(filter=\"resnet*\", pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚙️ Data Directory Path: /data/ephemeral/home/upstageailab-cv-classification-cv_5/data\n",
            "⚙️ Device : cuda\n",
            "⌚ 실험 시간: 2507022317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahyeon1\u001b[0m (\u001b[33mahyeon1-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📢 run name: 2507022317-resnet50.a1_in1k-opt_AdamW-img224-aug_0\n"
          ]
        }
      ],
      "source": [
        "project_root = '/data/ephemeral/home/upstageailab-cv-classification-cv_5'\n",
        "data_dir = os.path.join(project_root, 'data')\n",
        "print(\"⚙️ Data Directory Path:\", data_dir)\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "print(\"⚙️ Device :\",device)\n",
        "\n",
        "from zoneinfo import ZoneInfo\n",
        "from datetime import datetime\n",
        "CURRENT_TIME = datetime.now(ZoneInfo(\"Asia/Seoul\")).strftime(\"%y%m%d%H%M\")\n",
        "print(f\"⌚ 실험 시간: {CURRENT_TIME}\")\n",
        "\n",
        "CFG = {\n",
        "    'model_name': 'resnet50.a1_in1k', # timm model name\n",
        "    'pretrained': True, # timm pretrained 가중치 사용 여부\n",
        "    'fine_tuning': \"full\", # fine-tuning 방법론\n",
        "    # full : pretrained=True, pretrained가중치를 전부 재학습시킨다. \n",
        "    # head : pretrained=True, model backbone 부분은 freeze하고 head 부분을 재학습시킨다.\n",
        "    # custom : pretrained=True, backbone에서도 일부분을 재학습시킨다.\n",
        "    # scratch : pretrained=False, 모델 구조만 사용하고 모든 가중치를 처음부터 학습시킨다.\n",
        "    # Loss Function\n",
        "    'criterion': 'CrossEntropyLoss',\n",
        "    # optimizer name: SGD, RMSprop, Momentum, NAG, Adam, AdamW, NAdam, RAdam, Adafactor\n",
        "    # reference : https://www.notion.so/skier-song9/Pytorch-9cfee0520ed6468a94b024ea35e48018?source=copy_link#217c8d3f60f58044beeac55596433dc6\n",
        "    'optimizer_name': 'AdamW', \n",
        "    'lr': 1e-2, # learning rate\n",
        "    'weight_decay': 1e-4, # weight decay ratio\n",
        "    # scheduler_name : StepLR, ExponentialLR, CosineAnnealingLR, OneCycleLR, ReduceLROnPlateau\n",
        "    # reference : https://www.notion.so/skier-song9/Pytorch-9cfee0520ed6468a94b024ea35e48018?source=copy_link#1d2c8d3f60f58026b71ad399ead029a9\n",
        "    'scheduler_name': 'OneCycleLR',\n",
        "    \n",
        "    # 기타 변수들\n",
        "    'random_seed': 256, \n",
        "    'n_folds': 5, # validation set, cross-validation 시 fold의 개수\n",
        "    'val_split_ratio': 0.15, # train-val split 비율\n",
        "    'stratify': True, # validation set 분할 시 stratify 전략 사용 여부\n",
        "    'image_size': 224, # 만약 multi-scale train/test 시 None으로 설정\n",
        "\n",
        "    # normalization mean, std\n",
        "    # full file tuning 시 0.5가 유리\n",
        "    # pre-trained 모델 사용 시 pre-trained 모델의 mean, std를 사용\n",
        "    'norm_mean': [0.5, 0.5, 0.5],\n",
        "    'norm_std': [0.5, 0.5, 0.5],\n",
        "\n",
        "    # 적용하는 기법들 명시\n",
        "    'augmentation': {},\n",
        "\n",
        "    # 모델에 대한 hyperparameters\n",
        "    'model_layer': {}, # model layer의 filter size, activation function, pooling layer 등 변경 시 작성\n",
        "    'dropout': 0.3, # model layer에서 dropout 비율 변경 시 작성\n",
        "    'timm_activation': \"ReLU\",\n",
        "    'activation': \"SELU\", # ReLU, LeakyReLU, ELU, SELU, GELU, Tanh, PReLU, SiLU\n",
        "\n",
        "    # 학습 시 hyperparameters\n",
        "    'epochs': 10000, # max epoch\n",
        "    'patience': 50, # early stopping patience\n",
        "    'batch_size': 64,\n",
        "\n",
        "    # device\n",
        "    'device': device # device name\n",
        "}\n",
        "if CFG['patience'] < 5:\n",
        "    raise ValueError(\"Ealry stopping patience must be larger than 5!\")\n",
        "\n",
        "# wandb setting\n",
        "import wandb\n",
        "wandb.login()\n",
        "WB = False # wandb 기록할 거면 True, 아니면 False.\n",
        "project_name = \"upstage-img-clf\"\n",
        "# def get_runs(project_name):\n",
        "#     # wandb project의 runs들을 가져옴\n",
        "#     return wandb.Api().runs(path=project_name, order=\"-created_at\")\n",
        "# def get_latest_run(project_name):\n",
        "#     runs = get_runs(project_name) # wandb project의 runs들을 가져옴\n",
        "#     if not runs: # 최초 runs에는 project_name-001 로 이름을 만든다.\n",
        "#         return f\"{CFG['model_name']}-opt_{CFG['optimizer_name']}-img{CFG['image_size']}-aug_{len(CFG['augmentation'])}-{CURRENT_TIME}-0000\"\n",
        "#     return runs[0].name # 가장 최근의 runs 이름을 가져온다.\n",
        "# def auto_increment_run_suffix(name: str, pad=4):\n",
        "#     suffix = name.split(\"-\")[-1]\n",
        "#     next_suffix = str(int(suffix) + 1).zfill(pad) # 2 > 0002\n",
        "#     return name.replace(suffix, next_suffix)\n",
        "# recent_run_name = get_latest_run(project_name)\n",
        "# next_run_name = auto_increment_run_suffix(recent_run_name) # 가장 최근 runs의 숫자를 1 올린다.\n",
        "next_run_name = f\"{CURRENT_TIME}-{CFG['model_name']}-opt_{CFG['optimizer_name']}-img{CFG['image_size']}-aug_{len(CFG['augmentation'])}\"\n",
        "print(f\"📢 run name: {next_run_name}\")\n",
        "run = None\n",
        "if WB:\n",
        "    run = wandb.init(\n",
        "        project=project_name,\n",
        "        name=next_run_name,\n",
        "        config=CFG\n",
        "    )\n",
        "    pass\n",
        "\n",
        "# transform dictionary into namespace\n",
        "from types import SimpleNamespace\n",
        "CFG = SimpleNamespace(**CFG)\n",
        "\n",
        "# set random seed\n",
        "set_seed(CFG.random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download data\n",
        "# os.chdir(project_root)\n",
        "# !wget https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000356/data/data.tar.gz\n",
        "# !tar -zxvf data.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXa_FPM73R9f"
      },
      "source": [
        "## 2. Import Library & Define Functions\n",
        "* 학습 및 추론에 필요한 라이브러리를 로드합니다.\n",
        "* 학습 및 추론에 필요한 함수와 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hyl8oAy6TZAu"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 클래스를 정의합니다.\n",
        "class ImageDataset(Dataset):\n",
        "    # def __init__(self, csv, path, transform=None):\n",
        "    def __init__(self, df:pd.DataFrame, path, transform=None):\n",
        "        \"\"\"\n",
        "        :param pd.DataFrame df: train-val을 위해서는 pandas.Dataframe으로 받아야 함.\n",
        "        :param str path: 이미지 데이터 디렉토리 경로\n",
        "        :param _type_ transform: 이미지 변형, defaults to None\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name, target = self.df.iloc[idx]\n",
        "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)['image']\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class EarlyStopping:\n",
        "\tdef __init__(self, patience=5, min_delta=1e-6, restore_best_weights=True):\n",
        "\t\tself.patience = patience\n",
        "\t\tself.min_delta = min_delta\n",
        "\t\tself.restore_best_weights = restore_best_weights\n",
        "\t\tself.best_model_state_dict = None\n",
        "\t\tself.best_loss = None\n",
        "\t\tself.counter = 0\n",
        "\t\tself.status = \"\"\n",
        "\t\n",
        "\tdef __call__(self, model, val_loss):\n",
        "\t\tif self.best_loss is None:\n",
        "\t\t\t#현재의 모델로 self.best_loss, self.best_model_state_dict 업데이트\n",
        "\t\t\tself.best_loss = val_loss\n",
        "\t\t\tself.best_model_state_dict = copy.deepcopy(model.state_dict())\n",
        "\t\telif val_loss < self.best_loss - self.min_delta:\n",
        "\t\t\t#val_loss가 best_loss보다 좋을 때 > self.best_loss와 self.best_model 업데이트\n",
        "\t\t\tself.best_model_state_dict = copy.deepcopy(model.state_dict())\n",
        "\t\t\tself.best_loss = val_loss\n",
        "\t\t\tself.counter = 0\n",
        "\t\t\tself.status = f\"Improvement found, counter reset to {self.counter}\"\n",
        "\t\telse:\n",
        "\t\t\t#val_loss가 더 안 좋을 때 > patience 증가하고 early stop 여부 확인\n",
        "\t\t\tself.counter += 1\n",
        "\t\t\tself.status = f\"No improvement in the last {self.counter} epochs\"\n",
        "\t\t\tif self.counter >= self.patience: # ealy stop\n",
        "\t\t\t\tself.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
        "\t\t\t\tif self.restore_best_weights and self.best_model_state_dict is not None:\n",
        "\t\t\t\t\tmodel.load_state_dict(self.best_model_state_dict)\n",
        "\t\t\t\t\treturn True\n",
        "\t\treturn False # end with no early stop\n",
        "\t\n",
        "\tdef restore_best(self, model):\n",
        "\t\tif self.best_loss is not None and self.best_model_state_dict is not None:\n",
        "\t\t\tprint(f\"Restore model_state_dict of which best_loss: {self.best_loss:.6f}\")\n",
        "\t\t\tmodel.load_state_dict(self.best_model_state_dict)\n",
        "\t\t\treturn True\n",
        "\t\treturn False\n",
        "\n",
        "class TrainModule():\n",
        "\tdef __init__(self, model: torch.nn.Module, criterion, optimizer, scheduler, train_loader, valid_loader, cfg: SimpleNamespace, verbose:int =50, run=None):\n",
        "\t\t'''\n",
        "\t\tmodel, criterion, scheduler, train_loader, valid_loader 미리 정의해서 전달\n",
        "\t\tcfg : es_patience, epochs 등에 대한 hyperparameters를 namespace 객체로 입력\n",
        "\t\t'''\n",
        "\t\trequired_attrs = ['scheduler_name','patience', 'epochs']\n",
        "\t\tfor attr in required_attrs:\n",
        "\t\t\tassert hasattr(cfg, attr), f\"AttributeError: There's no '{attr}' attribute in cfg.\"\n",
        "\t\tassert verbose > 1 and verbose < cfg.epochs, f\"Logging frequency({verbose}) MUST BE smaller than EPOCHS({cfg.epochs}) and positive value.\"\n",
        "\t\t\n",
        "\t\tself.model = model\n",
        "\t\tself.criterion = criterion\n",
        "\t\tself.optimizer = optimizer\n",
        "\t\tself.scheduler = scheduler\n",
        "\t\tself.train_loader = train_loader\n",
        "\t\tself.valid_loader = valid_loader\n",
        "\t\tself.cfg = cfg\n",
        "\t\tif getattr(cfg, \"device\", False):\n",
        "\t\t\tself.model.to(self.cfg.device)\n",
        "\t\telse:\n",
        "\t\t\tself.cfg.device = 'cpu'\n",
        "\t\tself.es = EarlyStopping(patience=self.cfg.patience)\n",
        "\t\t### list for plot\n",
        "\t\tself.train_losses_for_plot, self.val_losses_for_plot = [], []\n",
        "\t\tself.train_acc_for_plot, self.val_acc_for_plot = [], [] # classification\n",
        "\t\tself.train_f1_for_plot, self.val_f1_for_plot = [], [] # classification\n",
        "\t\t# logging frequency\n",
        "\t\tself.verbose = verbose\n",
        "\t\t# wandb run object\n",
        "\t\tself.run = run\n",
        "\t\t\n",
        "\tdef training_step(self):\n",
        "\t\t# set train mode\n",
        "\t\tself.model.train()\n",
        "\t\trunning_loss = 0.0\n",
        "\t\tcorrect = 0 # classification\n",
        "\t\ttotal = 0\n",
        "\t\tall_preds = []\n",
        "\t\tall_targets = []\n",
        "\t\t\n",
        "\t\tfor train_x, train_y in self.train_loader: # batch training\n",
        "\t\t\ttrain_x, train_y = train_x.to(self.cfg.device), train_y.to(self.cfg.device)\n",
        "\t\t\t\n",
        "\t\t\tself.optimizer.zero_grad() # 이전 gradient 초기화\n",
        "\t\t\toutputs = self.model(train_x)\n",
        "\t\t\tloss = self.criterion(outputs, train_y)\n",
        "\t\t\t\n",
        "\t\t\tloss.backward() # backward pass\n",
        "\t\t\tself.optimizer.step() # 가중치 업데이트\n",
        "\t\t\tif self.cfg.scheduler_name == \"OneCycleLR\":\n",
        "\t\t\t\tself.scheduler.step()\n",
        "\t\t\t\n",
        "\t\t\trunning_loss += loss.item() * train_y.size(0) # train_loss \n",
        "\t\t\t_, predicted = torch.max(outputs, 1) # 가장 확률 높은 클래스 예측 # classification\n",
        "\t\t\tcorrect += (predicted == train_y).sum().item() # classification\n",
        "\t\t\ttotal += train_y.size(0) \n",
        "\n",
        "\t\t\tall_preds.extend(predicted.cpu().numpy())\n",
        "\t\t\tall_targets.extend(train_y.cpu().numpy())\n",
        "\t\t\t\n",
        "\t\tepoch_loss = running_loss / total # average loss of 1 epoch\n",
        "\t\tepoch_acc = 100 * correct / total # classification\n",
        "\t\tepoch_f1 = f1_score(all_targets, all_preds, average='macro') # classification\n",
        "\t\treturn epoch_loss, epoch_acc, epoch_f1  # classification\t\t\n",
        "\t\n",
        "\tdef validation_step(self):\n",
        "\t\tself.model.eval()  # 평가 모드\n",
        "\t\tval_loss = 0\n",
        "\t\tcorrect = 0 # classification\n",
        "\t\ttotal = 0\n",
        "\t\tall_preds = []\n",
        "\t\tall_targets = []\n",
        "\t\t\n",
        "\t\twith torch.no_grad():  # gradient 계산 비활성화\n",
        "\t\t\tfor val_x, val_y in self.valid_loader: # batch training\n",
        "\t\t\t\tval_x, val_y = val_x.to(self.cfg.device), val_y.to(self.cfg.device)\n",
        "\t\t\t\t\n",
        "\t\t\t\toutputs = self.model(val_x)\n",
        "\t\t\t\tloss = self.criterion(outputs, val_y)\n",
        "\t\t\t\t\t\t\t\t\n",
        "\t\t\t\tval_loss += loss.item() * val_y.size(0)\n",
        "\t\t\t\t_, predicted = torch.max(outputs, 1) # classification\n",
        "\t\t\t\tcorrect += (predicted == val_y).sum().item() # classification\n",
        "\t\t\t\ttotal += val_y.size(0)\n",
        "\n",
        "\t\t\t\tall_preds.extend(predicted.cpu().numpy())\n",
        "\t\t\t\tall_targets.extend(val_y.cpu().numpy())\n",
        "\t\t\n",
        "\t\tepoch_loss = val_loss / total # average loss of 1 epoch\n",
        "\t\tepoch_acc = 100 * correct / total # classification\n",
        "\t\tepoch_f1 = f1_score(all_targets, all_preds, average='macro') # classification\n",
        "\t\treturn epoch_loss, epoch_acc, epoch_f1 # classification\n",
        "\t\n",
        "\tdef training_loop(self):\n",
        "\t\ttry:\n",
        "\t\t\t# reset loss list for plots\n",
        "\t\t\tself.train_losses_for_plot, self.val_losses_for_plot = [], []\n",
        "\t\t\tself.train_acc_for_plot, self.val_acc_for_plot = [], []\n",
        "\t\t\tepoch_counter = 0\n",
        "\t\t\tepoch_timer = []\n",
        "\t\t\tdone = False\n",
        "\t\t\t\n",
        "\t\t\tpbar = tqdm(total=self.cfg.epochs)\n",
        "\t\t\twhile not done and epoch_counter<self.cfg.epochs:\n",
        "\t\t\t\tst = time.time()\n",
        "\t\t\t\tepoch_counter += 1\n",
        "\t\t\t\t\n",
        "\t\t\t\t# train\n",
        "\t\t\t\t# train_loss = self.training_step() # regression\n",
        "\t\t\t\ttrain_loss, train_acc, train_f1 = self.training_step() # classification\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.train_losses_for_plot.append(train_loss)\n",
        "\t\t\t\tself.train_acc_for_plot.append(train_acc) # classification\n",
        "\t\t\t\tself.train_f1_for_plot.append(train_f1) # classification\n",
        "\t\t\t\t\n",
        "\t\t\t\t# validation\n",
        "\t\t\t\t# val_loss = self.validation_step() # regression\n",
        "\t\t\t\tval_loss, val_acc, val_f1 = self.validation_step()  # classification\n",
        "\t\t\t\tself.val_losses_for_plot.append(val_loss)\n",
        "\t\t\t\tself.val_acc_for_plot.append(val_acc) # classification\n",
        "\t\t\t\tself.val_f1_for_plot.append(val_f1) # classification\n",
        "\n",
        "\t\t\t\t\n",
        "\t\t\t\t# scheduler의 종류에 따라 val_loss를 전달하거나 그냥 step() 호출.\n",
        "\t\t\t\tif self.cfg.scheduler_name == \"OneCycleLR\":\n",
        "\t\t\t\t\tpass\n",
        "\t\t\t\telif self.cfg.scheduler_name == \"ReduceLROnPlateau\":\n",
        "\t\t\t\t\tself.scheduler.step(val_loss)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tself.scheduler.step()\n",
        "\t\t\t\tepoch_timer.append(time.time() - st)\n",
        "\t\t\t\tpbar.update(1)\n",
        "\t\t\t\tif self.es(self.model, val_loss):\n",
        "\t\t\t\t\t# early stopped 된 경우 if 문 안으로 들어온다.\n",
        "\t\t\t\t\tdone = True\n",
        "\t\t\t\tif self.run is not None:\n",
        "\t\t\t\t\tprint('wandb logging...')\n",
        "\t\t\t\t\tepoch_log = {\n",
        "\t\t\t\t\t\t'train_loss': train_loss,\n",
        "\t\t\t\t\t\t'train_accuracy': train_acc,\n",
        "\t\t\t\t\t\t'train_f1': train_f1,\n",
        "\t\t\t\t\t\t'val_loss': val_loss,\n",
        "\t\t\t\t\t\t'val_accuracy': val_acc,\n",
        "\t\t\t\t\t\t'val_f1': val_f1\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\trun.log(epoch_log, step=epoch_counter) # wandb logging\n",
        "\t\t\t\tif epoch_counter == 1 or epoch_counter % self.verbose == 0:\n",
        "\t\t\t\t\t# self.verbose epoch마다 logging\n",
        "\t\t\t\t\tmean_time_spent = np.mean(epoch_timer)\n",
        "\t\t\t\t\tepoch_timer = [] # reset timer list\n",
        "\t\t\t\t\t# print(f\"Epoch {epoch_counter}/{self.cfg.epochs} [Time: {mean_time_spent:.2f}s], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.8f}\")\n",
        "\t\t\t\t\tprint(f\"Epoch {epoch_counter}/{self.cfg.epochs} [Time: {mean_time_spent:.2f}s], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.8f}\\n Train ACC: {train_acc:.2f}%, Validation ACC: {val_acc:.2f}%\\n Train F1: {train_f1:.4f}, Validation F1: {val_f1:.4f}\") # classification\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tprint(e)\n",
        "\t\t\treturn False # training loop failed\n",
        "\t\treturn True # training loop succeed\n",
        "\t\t\n",
        "\tdef plot_loss(self, show:bool=False, savewandb:bool=True, savedir:str=None):\n",
        "\t\t\"\"\"loss, accuracy, f1-score에 대한 그래프 시각화 함수\n",
        "\n",
        "\t\t:param bool show: plt.show()를 실행할 건지, defaults to False\n",
        "\t\t:param bool savewandb: wandb logging에 plot을 시각화하여 저장할 건지, defaults to True\n",
        "\t\t:param str savedir: plot을 저장할 디렉토리를 설정, None이면 저장 안 함, defaults to None\n",
        "\t\t:return _type_: None\n",
        "\t\t\"\"\"\n",
        "\t\timport matplotlib.pyplot as plt\n",
        "\t\tfig, ax = plt.subplots(figsize=(6, 4))\n",
        "\t\tplt.plot(range(len(self.train_losses_for_plot)),self.train_losses_for_plot,color='blue',label='train_loss')\n",
        "\t\tplt.plot(range(len(self.val_losses_for_plot)),self.val_losses_for_plot,color='red',label='val_loss')\n",
        "\t\tplt.axhline(y=1e-3, color='red', linestyle='--', label='(Overfit)')\n",
        "\t\tplt.legend()\n",
        "\t\tplt.xlabel(\"Epoch\")\n",
        "\t\tplt.ylabel(\"Loss\")\n",
        "\t\tplt.title(\"Train/Validation Loss plot\")\n",
        "\t\tif savedir is not None:\n",
        "\t\t\tif os.path.exists(savedir):\n",
        "\t\t\t\tos.makedirs(savedir, exist_ok=True)\n",
        "\t\t\tsavepath = os.path.join(savedir, \"loss_plot.png\")\n",
        "\t\t\tplt.savefig(savepath)\n",
        "\t\t\tprint(f\"⚙️loss plot saved in {savepath}\")\n",
        "\t\tif show:\n",
        "\t\t\tplt.show()\n",
        "\t\tif savewandb and self.run is not None:\n",
        "\t\t\trun.log({'loss_plot': wandb.Image(fig)}) # wandb\n",
        "\t\tplt.clf()\n",
        "\t\t\n",
        "\t\t# classification\n",
        "\t\tfig, ax = plt.subplots(figsize=(6, 4))\n",
        "\t\tplt.plot(range(len(self.train_acc_for_plot)),self.train_acc_for_plot,color='blue',label='train_acc')\n",
        "\t\tplt.plot(range(len(self.val_acc_for_plot)),self.val_acc_for_plot,color='red',label='val_acc')\n",
        "\t\tplt.axhline(y=99.0, color='red', linestyle='--', label='(99%)')\n",
        "\t\tplt.legend()\n",
        "\t\tplt.xlabel(\"Epoch\")\n",
        "\t\tplt.ylabel(\"Accuracy(%)\")\n",
        "\t\tplt.title(\"Train/Validation Accuracy Plot\")\n",
        "\t\tplt.grid()\n",
        "\t\tif savedir is not None:\n",
        "\t\t\tsavepath = os.path.join(savedir, \"accuracy_plot.png\")\n",
        "\t\t\tplt.savefig(savepath)\n",
        "\t\t\tprint(f\"⚙️accuracy plot saved in {savepath}\")\n",
        "\t\tif show:\n",
        "\t\t\tplt.show()\n",
        "\t\tif savewandb and self.run is not None:\n",
        "\t\t\trun.log({'accuracy_plot': wandb.Image(fig)}) # wandb\n",
        "\t\tplt.clf()\n",
        "\n",
        "\t\t# classification\n",
        "\t\tfig, ax = plt.subplots(figsize=(6, 4))\n",
        "\t\tplt.plot(range(len(self.train_f1_for_plot)),self.train_f1_for_plot,color='blue',label='train_f1')\n",
        "\t\tplt.plot(range(len(self.val_f1_for_plot)),self.val_f1_for_plot,color='red',label='val_f1')\n",
        "\t\tplt.axhline(y=0.99, color='red', linestyle='--', label='(0.99)')\n",
        "\t\tplt.legend()\n",
        "\t\tplt.xlabel(\"Epoch\")\n",
        "\t\tplt.ylabel(\"F1-score\")\n",
        "\t\tplt.title(\"Train/Validation F1-score Plot\")\n",
        "\t\tplt.grid()\n",
        "\t\tif savedir is not None:\n",
        "\t\t\tsavepath = os.path.join(savedir, \"f1_plot.png\")\n",
        "\t\t\tplt.savefig(savepath)\n",
        "\t\t\tprint(f\"⚙️f1 plot saved in {savepath}\")\n",
        "\t\tif show:\n",
        "\t\t\tplt.show()\n",
        "\t\tif savewandb and self.run is not None:\n",
        "\t\t\trun.log({'f1_plot': wandb.Image(fig)}) # wandb\n",
        "\t\tplt.clf()\n",
        "\t\treturn None\n",
        "\t\t\n",
        "\tdef save_experiments(self, savepath=None):\n",
        "\t\t\"\"\"\"\"\"\n",
        "\t\tsave_dict = {\n",
        "\t\t\t'model_state_dict': self.model.state_dict(),\n",
        "\t\t\t'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "\t\t\t'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "\t\t\t'cfg': vars(self.cfg) # 나중에 로드해서 CFG = SimpleNamespace(**cfg)로 복원\n",
        "\t\t}\n",
        "\t\tif savepath is not None:\n",
        "\t\t\tdirpath = os.path.dirname(savepath)\n",
        "\t\t\tif os.path.exists(dirpath):\n",
        "\t\t\t\tos.makedirs(dirpath, exist_ok=True)\n",
        "\t\t\ttorch.save(save_dict, f=savepath)\n",
        "\t\t\treturn True\n",
        "\t\treturn False\n",
        "\t\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amum-FlIojc6"
      },
      "source": [
        "## 3. Load Data & Augmentation\n",
        "* 학습, 테스트 데이터셋과 로더를 정의합니다.\n",
        "* 이미지 증강 기법을 정의합니다.\n",
        "\n",
        "> references   \n",
        "> - torchvision 직접 구현 : [예시 코드](https://skier-song9.notion.site/CustomDataset-Augmentation-1d1c8d3f60f58026b4def1895f3fb9b0?source=copy_link)   \n",
        "> - Albumentation : [예시 코드](https://albumentations.ai/docs/examples/)   \n",
        "> - Augraphy : [예시 코드](https://augraphy.readthedocs.io/en/latest/doc/source/example_usage.html), [PyTorch 프로세스에서 사용 방법](https://augraphy.readthedocs.io/en/latest/examples/pytorch_integration_classification_example.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "album_basic_transform = A.Compose(\n",
        "    [\n",
        "        A.SmallestMaxSize(max_size=CFG.image_size),\n",
        "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=1),\n",
        "        A.RandomCrop(height=CFG.image_size, width=CFG.image_size),\n",
        "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=1),\n",
        "        A.RandomBrightnessContrast(p=1),\n",
        "        A.Normalize(mean=CFG.norm_mean, std=CFG.norm_mean),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "llh5C7ZKoq2S"
      },
      "outputs": [],
      "source": [
        "# augmentation을 위한 transform 코드\n",
        "train_transform = A.Compose([\n",
        "    # 이미지 크기 조정\n",
        "    A.Resize(height=CFG.image_size, width=CFG.image_size),\n",
        "    # images normalization\n",
        "    A.Normalize(mean=CFG.norm_mean, std=CFG.norm_std),\n",
        "    # numpy 이미지나 PIL 이미지를 PyTorch 텐서로 변환\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# valid, test image 변환을 위한 transform 코드\n",
        "test_transform = A.Compose([\n",
        "    A.Resize(height=CFG.image_size, width=CFG.image_size),\n",
        "    A.Normalize(mean=CFG.norm_mean, std=CFG.norm_std),\n",
        "    ToTensorV2(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 훈련 데이터셋 증강"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train-validation split\n",
        "### TO-DO\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.int64'>\n"
          ]
        }
      ],
      "source": [
        "# 클래스 라벨의 타입 확인\n",
        "print(type(train_df.iloc[0, 1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = os.path.join(data_dir, \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 방법1(CROP or CUTOUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Crop 증강 파이프라인 설정\n",
        "# augment = A.Compose([\n",
        "#     # 이미지 크기 조정\n",
        "#     A.RandomResizedCrop(\n",
        "#         height=CFG.image_size, width=CFG.image_size,\n",
        "#         scale=(0.85, 0.95),  # 최소 85%, 최대 95% 영역을 crop\n",
        "#         ratio=(0.9, 1.1),    # 종횡비 유지, 약간의 비율 변화 허용\n",
        "#         p=1.0\n",
        "#     )\n",
        "# ])\n",
        "\n",
        "# Cutout 증강 파이프라인 설정\n",
        "augment = A.Compose([\n",
        "    # 이미지 크기 조정\n",
        "    A.Cutout(\n",
        "        num_holes=1,                  # 마스킹 영역 개수\n",
        "        max_h_size=int(CFG.image_size * 0.2),   # 높이 최대 10% (예: 224px이면 22px)\n",
        "        max_w_size=int(CFG.image_size * 0.4),   # 너비 최대 20% (예: 224px이면 44px)\n",
        "        fill_value=0,                 # 검정색 박스 마스킹\n",
        "        p=1                         # 50% 확률로 적용\n",
        "    )\n",
        "])\n",
        "\n",
        "# 증강 대상 클래스\n",
        "augment_classes = [1, 13, 14]\n",
        "max_samples = 90\n",
        "\n",
        "# 증강 이미지, 라벨, ID 리스트 초기화\n",
        "augmented_labels = []    # 증강된 이미지 라벨\n",
        "augmented_ids = []       # 증강된 이미지 ID\n",
        "\n",
        "# 증강 대상 클래스 루프\n",
        "for cls in augment_classes:\n",
        "    cls_df = train_df[train_df['target'] == cls]\n",
        "    current_count = len(cls_df)\n",
        "\n",
        "    if current_count * 2 < max_samples:\n",
        "        # 모든 이미지 1회씩 증강\n",
        "        for idx, row in cls_df.iterrows():\n",
        "            img_id = row['ID']\n",
        "            img_path = os.path.join(data_dir, img_id)\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            augmented = augment(image=img)['image']\n",
        "\n",
        "            new_id = f\"aug_{img_id}\"\n",
        "            save_path = os.path.join(data_dir, new_id)\n",
        "\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "            augmented_labels.append(cls)\n",
        "            augmented_ids.append(new_id)\n",
        "\n",
        "    else:\n",
        "        to_generate = max_samples - current_count\n",
        "        \n",
        "        if to_generate <= 0:\n",
        "            continue\n",
        "\n",
        "        # 한 번에 to_generate 만큼 랜덤 샘플링\n",
        "        sampled_df = cls_df.sample(n=to_generate, replace=False, random_state=42).reset_index(drop=True)\n",
        "        \n",
        "        for i, row in sampled_df.iterrows():\n",
        "            img_id = row['ID']\n",
        "\n",
        "            # 이미지 로드\n",
        "            img_path = f\"{data_dir}/{img_id}\"\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # 증강\n",
        "            augmented = augment(image=img)['image']\n",
        "\n",
        "            # 저장할 파일명 및 경로\n",
        "            new_id = f\"aug_{img_id}\"\n",
        "            save_path = os.path.join(data_dir, new_id)\n",
        "\n",
        "            # RGB → BGR 변환 후 저장\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "            augmented_labels.append(cls)          # 라벨\n",
        "            augmented_ids.append(new_id)          # ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 방법1-1(증강 적용 후 또 증강 적용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 증강 파이프라인 설정\n",
        "# augment = A.Compose([\n",
        "#     A.Cutout(\n",
        "#         num_holes=1,\n",
        "#         max_h_size=int(CFG.image_size * 0.1),\n",
        "#         max_w_size=int(CFG.image_size * 0.2),\n",
        "#         fill_value=0,\n",
        "#         p=1\n",
        "#     )\n",
        "# ])\n",
        "\n",
        "# augment_classes = [1, 13, 14]\n",
        "# max_samples = 100\n",
        "\n",
        "# augmented_labels = []\n",
        "# augmented_ids = []\n",
        "\n",
        "# for cls in augment_classes:\n",
        "#     cls_df = train_df[train_df['target'] == cls].reset_index(drop=True)\n",
        "#     current_count = len(cls_df)\n",
        "\n",
        "#     if current_count * 2 < max_samples:\n",
        "#         # 모든 이미지 1회씩 증강\n",
        "#         for idx, row in cls_df.iterrows():\n",
        "#             img_id = row['ID']\n",
        "#             img_path = os.path.join(data_dir, img_id)\n",
        "#             img = cv2.imread(img_path)\n",
        "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#             augmented = augment(image=img)['image']\n",
        "\n",
        "#             new_id = f\"aug_{img_id}\"\n",
        "#             save_path = os.path.join(data_dir, new_id)\n",
        "\n",
        "#             cv2.imwrite(save_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "#             augmented_labels.append(cls)\n",
        "#             augmented_ids.append(new_id)\n",
        "\n",
        "#         # 부족한 수 만큼 증강된 이미지에서 추가 증강\n",
        "#         additional_needed = max_samples - (current_count * 2)\n",
        "#         augmented_df = pd.DataFrame({'ID': augmented_ids[-current_count:], 'target': [cls]*current_count})\n",
        "\n",
        "#         sampled_df = augmented_df.sample(n=additional_needed, replace=True, random_state=42).reset_index(drop=True)\n",
        "\n",
        "#         for idx, row in sampled_df.iterrows():\n",
        "#             img_id = row['ID']\n",
        "#             img_path = os.path.join(data_dir, img_id)\n",
        "#             img = cv2.imread(img_path)\n",
        "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#             augmented = augment(image=img)['image']\n",
        "\n",
        "#             new_id = f\"aug2_{img_id}\"\n",
        "#             save_path = os.path.join(data_dir, new_id)\n",
        "\n",
        "#             cv2.imwrite(save_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "#             augmented_labels.append(cls)\n",
        "#             augmented_ids.append(new_id)\n",
        "\n",
        "#     else:\n",
        "#         # 기존 로직: 부족한 만큼만 증강\n",
        "#         to_generate = max_samples - current_count\n",
        "#         if to_generate <= 0:\n",
        "#             continue\n",
        "\n",
        "#         sampled_df = cls_df.sample(n=to_generate, replace=False, random_state=42).reset_index(drop=True)\n",
        "\n",
        "#         for idx, row in sampled_df.iterrows():\n",
        "#             img_id = row['ID']\n",
        "#             img_path = os.path.join(data_dir, img_id)\n",
        "#             img = cv2.imread(img_path)\n",
        "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#             augmented = augment(image=img)['image']\n",
        "\n",
        "#             new_id = f\"aug_{img_id}\"\n",
        "#             save_path = os.path.join(data_dir, new_id)\n",
        "\n",
        "#             cv2.imwrite(save_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "#             augmented_labels.append(cls)\n",
        "#             augmented_ids.append(new_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 방법2(MIXUP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def mixup_images_numpy(img1, img2, alpha=0.4):\n",
        "#     lam = np.random.beta(alpha, alpha)\n",
        "#     mixed = lam * img1.astype(np.float32) + (1 - lam) * img2.astype(np.float32)\n",
        "#     mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n",
        "#     return mixed\n",
        "\n",
        "# augment_classes = [1, 13, 14]\n",
        "# max_samples = 74\n",
        "\n",
        "# augmented_labels = []\n",
        "# augmented_ids = []\n",
        "\n",
        "# for cls in augment_classes:\n",
        "#     cls_df = train_df[train_df['target'] == cls].reset_index(drop=True)\n",
        "#     current_count = len(cls_df)\n",
        "#     to_generate = max_samples - current_count\n",
        "\n",
        "#     if to_generate <= 0:\n",
        "#         continue\n",
        "\n",
        "#     # ID 리스트\n",
        "#     id_list = cls_df['ID'].tolist()\n",
        "\n",
        "#     # 가능한 모든 (id1, id2) 조합 생성, 자기 자신과 조합 제외\n",
        "#     possible_pairs = list(itertools.combinations(id_list, 2))\n",
        "\n",
        "#     # to_generate 만큼 랜덤 샘플링\n",
        "#     sampled_pairs = random.sample(possible_pairs, min(to_generate, len(possible_pairs)))\n",
        "\n",
        "#     for id1, id2 in sampled_pairs:\n",
        "#         # 이미지 1\n",
        "#         img_path1 = os.path.join(data_dir, id1)\n",
        "#         img1 = cv2.imread(img_path1)\n",
        "#         img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "#         img1 = cv2.resize(img1, (443, 591), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "#         # 이미지 2\n",
        "#         img_path2 = os.path.join(data_dir, id2)\n",
        "#         img2 = cv2.imread(img_path2)\n",
        "#         img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "#         img2 = cv2.resize(img2, (443, 591), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "#         # mixup\n",
        "#         mixed_img = mixup_images_numpy(img1, img2, alpha=0.4)\n",
        "\n",
        "#         name1 = os.path.splitext(id1)[0]\n",
        "#         name2 = os.path.splitext(id2)[0]\n",
        "\n",
        "#         # 저장할 파일명 및 경로\n",
        "#         new_id = f\"aug_{name1}_{name2}.jpg\"\n",
        "#         save_path = os.path.join(data_dir, new_id)  # 필요시 aug_dir로 변경 가능\n",
        "\n",
        "#         # RGB → BGR 후 저장\n",
        "#         cv2.imwrite(save_path, cv2.cvtColor(mixed_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "#         augmented_labels.append(cls)\n",
        "#         augmented_ids.append(new_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = os.path.join(project_root, 'data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 증강된 데이터용 ID와 라벨만 DataFrame 생성 (이미지 배열 제외)\n",
        "aug_df = pd.DataFrame({\n",
        "    'ID': augmented_ids,      # 이미지 배열이 아니라 ID 리스트 사용\n",
        "    'target': augmented_labels\n",
        "})\n",
        "\n",
        "# 기존 데이터와 증강 데이터 병합 (이미지는 별도 관리)\n",
        "combined_train_df = pd.concat([train_df, aug_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train-Validation 분할"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 증강 미 진행 시 활성화\n",
        "# combined_train_df = train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1419 251\n"
          ]
        }
      ],
      "source": [
        "train_idx, val_idx = train_test_split(\n",
        "    range(len(combined_train_df)),\n",
        "    test_size=CFG.val_split_ratio,\n",
        "    stratify=combined_train_df['target'],\n",
        "    shuffle=True,\n",
        "    random_state=CFG.random_seed\n",
        ")\n",
        "print(len(train_idx), len(val_idx))\n",
        "\n",
        "# Create train and validation CSV files temporarily\n",
        "train_subset = combined_train_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_subset = combined_train_df.iloc[val_idx].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset 및 DataLoader 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INxdmsStop2L",
        "outputId": "49f0d412-8ce6-4d2f-ae78-d5cf3d056340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1419 251 3140\n"
          ]
        }
      ],
      "source": [
        "# Dataset 정의\n",
        "train_dataset = ImageDataset(\n",
        "    train_subset,\n",
        "    os.path.join(data_dir, \"train\"),\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "# Augmented Dataset 정의\n",
        "album_basic_dataset = ImageDataset(\n",
        "    train_subset,\n",
        "    os.path.join(data_dir, \"train\"),\n",
        "    transform=album_basic_transform\n",
        ")\n",
        "\n",
        "# Concat Train Datasets\n",
        "combined_dataset = ConcatDataset(\n",
        "    [train_dataset, album_basic_dataset]\n",
        ")\n",
        "\n",
        "\n",
        "valid_dataset = ImageDataset(\n",
        "    val_subset,\n",
        "    os.path.join(data_dir, \"train\"),\n",
        "    transform=test_transform\n",
        ")\n",
        "\n",
        "test_dataset = ImageDataset(\n",
        "    pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\")),\n",
        "    os.path.join(data_dir, \"test\"),\n",
        "    transform=test_transform\n",
        ")\n",
        "print(len(train_dataset), len(valid_dataset), len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_sO03fWaQj1h"
      },
      "outputs": [],
      "source": [
        "# DataLoader 정의\n",
        "train_loader = DataLoader(\n",
        "    combined_dataset,\n",
        "    batch_size=CFG.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=CFG.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CFG.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "for train_x, train_y in train_loader:\n",
        "    print(train_x.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmm5h3J-pXNV"
      },
      "source": [
        "## 5. Train Model\n",
        "* 모델을 로드하고, 학습을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_activation(CFG):\n",
        "    ACTIVATIONS = {\n",
        "        'ReLU': nn.ReLU, # 음수는 0으로, 양수는 선형함수\n",
        "        'LeakyReLU': nn.LeakyReLU, # 음수도 일부 통과 -> Dead ReLU 방지\n",
        "        'ELU': nn.ELU, # 음수도 일부 통과 → 출력 평균이 0에 가깝도록 함으로써 학습 안정화 도움\n",
        "        'SELU': nn.SELU, # ELU에 스케일링 계수를 곱해 신경망을 자기 정규화, 특정 조건 (예: fully connected, 특정 초기화, 특정 구조)에서만 자기 정규화 효과가 잘 발휘\n",
        "        'GELU': nn.GELU, # 더 부드러운 비선형성, Transformer, BERT류\n",
        "        'Tanh': nn.Tanh, # 완만한 sigmoid\n",
        "        'PReLU': nn.PReLU, # 음수도 일부 통과 -> Dead ReLU 방지, 기울기를 학습함.\n",
        "        'SiLU': nn.SiLU, # 더 부드러운 비선형성, EfficientNet, Swin Transformer 등\n",
        "    }\n",
        "    return ACTIVATIONS[CFG.activation]\n",
        "\n",
        "class TimmWrapper(nn.Module):\n",
        "    def __init__(self, CFG):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name=CFG.model_name,\n",
        "            pretrained=CFG.pretrained,\n",
        "            num_classes=0, global_pool='avg'\n",
        "        )\n",
        "        self.dropout = nn.Dropout(p=CFG.dropout)\n",
        "        self.activation = get_activation(CFG)()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.backbone.num_features, 1000),\n",
        "            nn.BatchNorm1d(1000),\n",
        "            self.activation,\n",
        "            self.dropout,\n",
        "            nn.Linear(1000, 17)\n",
        "        )\n",
        "        def weight_init(m):\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                if CFG.timm_activation in ['Tanh']:\n",
        "                    # Xavier 초기화\n",
        "                    init.xavier_uniform_(m.weight)\n",
        "                else:\n",
        "                    # He 초기화\n",
        "                    init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "        if CFG.fine_tuning == 'head':\n",
        "            # backbone 파라미터를 freeze\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "            # classifier는 가중치 초기화\n",
        "            self.classifier.apply(weight_init)\n",
        "        elif CFG.fine_tuning == 'custom':\n",
        "            # 직접 커스터마이징\n",
        "            pass\n",
        "        elif CFG.fine_tuning == 'scratch':\n",
        "            self.apply(weight_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def get_timm_model(CFG):\n",
        "    if CFG.dropout is not None:\n",
        "        return TimmWrapper(CFG).to(CFG.device)\n",
        "\n",
        "    return timm.create_model(\n",
        "        CFG.model_name,\n",
        "        pretrained=CFG.pretrained,\n",
        "        num_classes=17,\n",
        "        act_layer=CFG.timm_activation # activation function 설정 (pretrained=True라면 pretrained의 activation을 그대로 사용하는 것이 좋다.)\n",
        "    ).to(CFG.device)\n",
        "\n",
        "def get_criterion(CFG):\n",
        "    CRITERIONS = {\n",
        "        \"CrossEntropyLoss\" : nn.CrossEntropyLoss()\n",
        "    }\n",
        "    return CRITERIONS[CFG.criterion]\n",
        "\n",
        "def get_optimizers(model, CFG):\n",
        "    # SGD, RMSprop, Momentum, NAG, Adam, AdamW, NAdam, RAdam, Adafactor\n",
        "    OPTIMIZERS = {\n",
        "        'SGD': optim.SGD(model.parameters(), lr=CFG.lr),\n",
        "        'RMSprop': optim.RMSprop(model.parameters(), lr=CFG.lr, alpha=0.99, weight_decay=CFG.weight_decay),\n",
        "        'Momentum': optim.SGD(model.parameters(), lr=CFG.lr, momentum=0.9),\n",
        "        'NAG' : optim.SGD(model.parameters(), lr=CFG.lr, momentum=0.9, nesterov=True),\n",
        "        'Adam' : optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay),\n",
        "        'AdamW': optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay),\n",
        "        'NAdam': optim.NAdam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, momentum_decay=4e-3),\n",
        "        'RAdam': optim.RAdam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay),\n",
        "        'Adafactor': optim.Adafactor(model.parameters(), lr=CFG.lr, beta2_decay=-0.8, d=1.0, weight_decay=CFG.weight_decay, maximize=False)\n",
        "    }\n",
        "    return OPTIMIZERS[CFG.optimizer_name]\n",
        "\n",
        "def get_scheduler(optimizer, CFG):\n",
        "    # StepLR, ExponentialLR, CosineAnnealingLR, OneCycleLR, ReduceLROnPlateau\n",
        "    SCHEDULERS = {\n",
        "        'StepLR': lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1),\n",
        "        'ExponentialLR': lr_scheduler.ExponentialLR(optimizer, gamma=0.95),\n",
        "        'CosineAnnealingLR': lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.epochs, eta_min=0),\n",
        "        'OneCycleLR': lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=CFG.epochs),\n",
        "        'ReduceLROnPlateau': lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=CFG.patience-5, min_lr=0),\n",
        "    }\n",
        "    return SCHEDULERS[CFG.scheduler_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FbBgFPsLT-CO"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "model = get_timm_model(CFG)\n",
        "\n",
        "criterion = get_criterion(CFG)\n",
        "\n",
        "# define optimizer\n",
        "# SGD, RMSprop, Momentum, NAG, Adam, AdamW, NAdam, RAdam, Adafactor\n",
        "optimizer = get_optimizers(model, CFG)\n",
        "\n",
        "# define scheduler\n",
        "# StepLR, ExponentialLR, CosineAnnealingLR, OneCycleLR, ReduceLROnPlateau\n",
        "scheduler = get_scheduler(optimizer, CFG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = TrainModule(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    train_loader=train_loader,\n",
        "    valid_loader=valid_loader,\n",
        "    cfg=CFG,\n",
        "    verbose=15,\n",
        "    run=run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TimmWrapper(\n",
              "  (backbone): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act1): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (drop_block): Identity()\n",
              "        (act2): ReLU(inplace=True)\n",
              "        (aa): Identity()\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act3): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
              "    (fc): Identity()\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (activation): SELU()\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=1000, bias=True)\n",
              "    (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): SELU()\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Linear(in_features=1000, out_features=17, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/10000 [00:08<22:46:39,  8.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10000 [Time: 8.20s], Train Loss: 1.1579, Validation Loss: 0.51033927\n",
            " Train ACC: 62.54%, Validation ACC: 82.47%\n",
            " Train F1: 0.6164, Validation F1: 0.8206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 15/10000 [01:54<21:05:51,  7.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/10000 [Time: 7.57s], Train Loss: 0.0755, Validation Loss: 0.18062885\n",
            " Train ACC: 97.25%, Validation ACC: 93.23%\n",
            " Train F1: 0.9725, Validation F1: 0.9282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 30/10000 [03:48<21:04:15,  7.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/10000 [Time: 7.59s], Train Loss: 0.0358, Validation Loss: 0.26003829\n",
            " Train ACC: 98.73%, Validation ACC: 92.83%\n",
            " Train F1: 0.9872, Validation F1: 0.9223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 45/10000 [05:42<21:04:12,  7.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45/10000 [Time: 7.60s], Train Loss: 0.0277, Validation Loss: 0.26618188\n",
            " Train ACC: 98.84%, Validation ACC: 94.82%\n",
            " Train F1: 0.9885, Validation F1: 0.9452\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 46/10000 [05:49<21:02:51,  7.61s/it]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# wandb watch >> layer가 깊으면 어차피 시각화해서 보기 어려움;;\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# if WB:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     run.watch(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# run train\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[8], line 160\u001b[0m, in \u001b[0;36mTrainModule.training_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_f1_for_plot\u001b[38;5;241m.\u001b[39mappend(train_f1) \u001b[38;5;66;03m# classification\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# val_loss = self.validation_step() # regression\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m val_loss, val_acc, val_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# classification\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_losses_for_plot\u001b[38;5;241m.\u001b[39mappend(val_loss)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_acc_for_plot\u001b[38;5;241m.\u001b[39mappend(val_acc) \u001b[38;5;66;03m# classification\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[8], line 117\u001b[0m, in \u001b[0;36mTrainModule.validation_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m all_targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# gradient 계산 비활성화\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m val_x, val_y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_loader: \u001b[38;5;66;03m# batch training\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \t\tval_x, val_y \u001b[38;5;241m=\u001b[39m val_x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice), val_y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    120\u001b[0m \t\toutputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(val_x)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, name)))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 21\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, target\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/composition.py:210\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    207\u001b[0m     p\u001b[38;5;241m.\u001b[39mpreprocess(data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transforms):\n\u001b[0;32m--> 210\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_each_transform:\n\u001b[1;32m    213\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data_post_transform(data)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/transforms_interface.py:118\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             warn(\n\u001b[1;32m    114\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_class_fullname() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m could work incorrectly in ReplayMode for other input data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m because its\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m params depend on targets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m deepcopy(params)\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/transforms_interface.py:131\u001b[0m, in \u001b[0;36mBasicTransform.apply_with_params\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     target_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_function(key)\n\u001b[1;32m    130\u001b[0m     target_dependencies \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_dependence\u001b[38;5;241m.\u001b[39mget(key, [])}\n\u001b[0;32m--> 131\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtarget_dependencies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/geometric/resize.py:184\u001b[0m, in \u001b[0;36mResize.apply\u001b[0;34m(self, img, interpolation, **params)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_LINEAR, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/utils.py:122\u001b[0m, in \u001b[0;36mpreserve_channel_dim.<locals>.wrapped_function\u001b[0;34m(img, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_function\u001b[39m(img: np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    121\u001b[0m     shape \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 122\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    124\u001b[0m         result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(result, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/geometric/functional.py:392\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, height, width, interpolation)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[1;32m    391\u001b[0m resize_fn \u001b[38;5;241m=\u001b[39m _maybe_process_in_chunks(cv2\u001b[38;5;241m.\u001b[39mresize, dsize\u001b[38;5;241m=\u001b[39m(width, height), interpolation\u001b[38;5;241m=\u001b[39minterpolation)\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/utils.py:208\u001b[0m, in \u001b[0;36m_maybe_process_in_chunks.<locals>.__process_fn\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    206\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdstack(chunks)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# wandb watch >> layer가 깊으면 어차피 시각화해서 보기 어려움;;\n",
        "# if WB:\n",
        "#     run.watch(\n",
        "#         models=trainer.model,\n",
        "#         criterion=trainer.criterion,\n",
        "#         log='all',\n",
        "#         log_graph=True\n",
        "#     )\n",
        "# run train\n",
        "trainer.training_loop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkwxRXoBpbaX"
      },
      "source": [
        "# 6. Inference & Save File\n",
        "* 테스트 이미지에 대한 추론을 진행하고, 결과 파일을 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 저장된 trainer를 로드하여 추론하는 경우 아래 함수로 로드\n",
        "def load_checkpoint_model(savepath=None):\n",
        "    if os.path.exists(savepath):\n",
        "        checkpoint = torch.load(savepath)\n",
        "        cfg = SimpleNamespace(**checkpoint['cfg'])\n",
        "        model = get_timm_model(cfg)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        return model, cfg\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "# model, cfg = load_checkpoint_model(\n",
        "#     savepath=os.path.join(project_root,'models','resnetrs101.tf_in1k-opt_Adam-img224-aug_0-2506301324-0001.pth')\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRYe6jlPU_Om",
        "outputId": "2a08690c-9ffe-418d-8679-eb9280147110"
      },
      "outputs": [],
      "source": [
        "preds_list = []\n",
        "\n",
        "trainer.model.eval()\n",
        "for image, _ in tqdm(test_loader):\n",
        "    image = image.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = trainer.model(image)\n",
        "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
        "\n",
        "### checkpoint로 로드한 모델로 추론하는 경우\n",
        "# model.eval()\n",
        "# for image, _ in tqdm(test_loader):\n",
        "#     image = image.to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         preds = model(image)\n",
        "#     preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aClN7Qi7VZoh"
      },
      "outputs": [],
      "source": [
        "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
        "pred_df['target'] = preds_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDBXQqAzVvLY"
      },
      "outputs": [],
      "source": [
        "sample_submission_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\n",
        "assert (sample_submission_df['ID'] == pred_df['ID']).all(), \"pred_df에서 test 이미지가 아닌 데이터가 존재합니다.\"\n",
        "assert set(pred_df['target']).issubset(set(range(17))), \"target 컬럼에 0~16 외의 값이 있습니다.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'저장할 Submission 파일 이름: {next_run_name}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### submission 파일 저장\n",
        "pred_df.to_csv(os.path.join(data_dir, 'submissions', f'{next_run_name}.csv'), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### trainer 저장\n",
        "trainer_savepath = os.path.join(project_root, \"models\", f\"{next_run_name}.pth\")\n",
        "trainer.save_experiments(savepath=trainer_savepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 학습 loss plot & 추론 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePx2vCELVnuS"
      },
      "outputs": [],
      "source": [
        "# loss, accuracy, f1 plot 시각화 & wandb 저장\n",
        "trainer.plot_loss(\n",
        "    show=True,\n",
        "    savewandb=True,\n",
        "    savedir=None # 저장 안 함\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9yMO8s6GqAwZ",
        "outputId": "9a30616f-f0ea-439f-a906-dd806737ce00"
      },
      "outputs": [],
      "source": [
        "display(pred_df[pred_df['ID']=='0a4f2decf34d3bff.jpg'])\n",
        "display(pred_df[pred_df['ID']=='0a12d28777501f71.jpg'])\n",
        "display(pred_df[pred_df['ID']=='0b60e9d39b43e0b9.jpg'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. WandB Finish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# train 폴더에 저장된 증강 이미지 삭제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = os.path.join(project_root, 'data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_dir = f'{data_dir}/train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for filename in os.listdir(target_dir):\n",
        "    if filename.startswith(\"aug_\"):\n",
        "        file_path = os.path.join(target_dir, filename)\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to delete {file_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
