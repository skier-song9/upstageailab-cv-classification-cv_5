# Configuration for the training process
model_name: 'swin_base_patch4_window12_384.ms_in1k' # timm model name
pretrained: True # timm pretrained 가중치 사용 여부
fine_tuning: "full" # fine-tuning 방법론
  # full : pretrained=True, pretrained가중치를 전부 재학습시킨다. 
  # head : pretrained=True, model backbone 부분은 freeze하고 head 부분을 재학습시킨다.
  # custom : pretrained=True, backbone에서도 일부분을 재학습시킨다.
  # scratch : pretrained=False, 모델 구조만 사용하고 모든 가중치를 처음부터 학습시킨다.

# Loss Function
criterion: 'CrossEntropyLoss'

# Optimizer
# optimizer name: SGD, RMSprop, Momentum, NAG, Adam, AdamW, NAdam, RAdam, Adafactor
# reference : https://www.notion.so/skier-song9/Pytorch-9cfee0520ed6468a94b024ea35e48018?source=copy_link#217c8d3f60f58044beeac55596433dc6
optimizer_name: 'AdamW'
# optimizer_params:
lr: 0.0001 # 1e-4
weight_decay: 0.00001 # 1e-5

# Scheduler
# scheduler_name : StepLR, ExponentialLR, CosineAnnealingLR, OneCycleLR, ReduceLROnPlateau
# reference : https://www.notion.so/skier-song9/Pytorch-9cfee0520ed6468a94b024ea35e48018?source=copy_link#1d2c8d3f60f58026b71ad399ead029a9    
scheduler_name: 'CosineAnnealingLR'
# scheduler_params:
#   max_lr: 0.01 # for OneCycleLR
  # step_size: 50 # for StepLR
  # gamma: 0.1 # for StepLR, ExponentialLR
  # T_max: 10000 # for CosineAnnealingLR
  # mode: 'min' # for ReduceLROnPlateau
  # factor: 0.1 # for ReduceLROnPlateau
  # min_lr: 0 # for ReduceLROnPlateau

# Other variables
random_seed: 256
n_folds: 0 # number of folds for cross-validation
val_split_ratio: 0.15 # train-val split 비율
stratify: True # validation set 분할 시 stratify 전략 사용 여부
image_size: 384 # 만약 multi-scale train/test 시 None으로 설정

# Normalization
# full file tuning 시 0.5가 유리
# pre-trained 모델 사용 시 pre-trained 모델의 mean, std를 사용
norm_mean: [0.5, 0.5, 0.5]
norm_std: [0.5, 0.5, 0.5]

# Techniques
class_imbalance: 
  aug_class: [1, 13, 14]
  max_samples: 78
online_augmentation: True
augmentation: # normal augmentation : dynamic augmentation이 활성화되어 있으면 일반 augmentation은 자동으로 비활성화된다.
  eda: True
  dilation: False
  erosion: False
  mixup: False
  cutmix: False
# training epoch에 따라 동적으로 증강기법을 변환하는 방법
dynamic_augmentation:
  enabled: True
  policies:
    weak:
      end_epoch: 5
      augs: ['basic']
    middle:
      end_epoch: 15
      augs: ['middle']
    strong:
      end_epoch: 300
      augs: ['aggressive','eda']
  # you can add more augmentations here, e.g.,
  # random_crop: True
  # color_jitter: True
val_TTA: True # Validation 시, Test Time Augmentation 사용 여부
test_TTA: True # Inference 시, Test Time Augmentation 사용 여부
tta_dropout: False # inference 시에도 model.train() 모드를 사용해 dropout을 활성화하는 방법
mixed_precision: True # Mixed Precision 학습 사용 여부 > 사용하면 더 큰 batch_size 학습 가능

# Model hyperparameters
timm:
  activation: None # ReLU, LeakyReLU, ELU, SELU, GELU, Tanh, PReLU, SiLU
  # None 입력시 timm 모델의 기본 activation 사용
  # dropout과 같이 추가적인 옵션이 존재하는 경우, key: value 형태로 전달.
custom_layer: # custom classifier head를 사용하고 싶은 경우 설정. None일 때 custom head 사용 안 함. 
  # head_type: "simple_dropout"
  # drop: 0.2
  # activation: 'GELU'

# Training hyperparameters
epochs: 10000 # max epoch
patience: 20 # early stopping patience
batch_size: 32 # image_size, model_size, GPU RAM 에 따라 OOM이 발생하지 않도록 설정.

# W&B
wandb:
  project: "upstage-img-clf"
  log: False # log using wandb, if False then do not use wandb

# Paths
data_dir: "/data/ephemeral/home/upstageailab-cv-classification-cv_5/data"
