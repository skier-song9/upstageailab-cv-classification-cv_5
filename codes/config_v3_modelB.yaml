# Configuration for the training process
model_name: 'maxvit_base_tf_384.in21k_ft_in1k' # timm model name
pretrained: True # timm pretrained ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€
fine_tuning: "full" # fine-tuning ë°©ë²•ë¡ 
  # full : pretrained=True, pretrainedê°€ì¤‘ì¹˜ë¥¼ ì „ë¶€ ì¬í•™ìŠµì‹œí‚¨ë‹¤. 
  # head : pretrained=True, model backbone ë¶€ë¶„ì€ freezeí•˜ê³  head ë¶€ë¶„ì„ ì¬í•™ìŠµì‹œí‚¨ë‹¤.
  # custom : pretrained=True, backboneì—ì„œë„ ì¼ë¶€ë¶„ì„ ì¬í•™ìŠµì‹œí‚¨ë‹¤.
  # scratch : pretrained=False, ëª¨ë¸ êµ¬ì¡°ë§Œ ì‚¬ìš©í•˜ê³  ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¨ë‹¤.

# Loss Function
criterion: 'FocalLoss' # CrossEntropyLoss, FocalLoss, LabelSmoothingLoss
class_weighting: False # classì— ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ Loss ê³„ì‚°
label_smooth: 0.0
# Optimizer
# optimizer name: SGD, RMSprop, Momentum, NAG, Adam, AdamW, NAdam, RAdam, Adafactor
# reference : https://www.notion.so/skier-song9/Pytorch-9cfee0520ed6468a94b024ea35e48018?source=copy_link#217c8d3f60f58044beeac55596433dc6
optimizer_name: 'Adafactor'
lr: 0.0001 # 1e-3 ~ 1e-4 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì‹œì‘
weight_decay: 0.00001 # 1e-2 ~ 1e-5 ì‚¬ì´ì˜ ê°’ì´ ì¼ë°˜ì 

# Scheduler
# scheduler_name : StepLR, ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmupRestarts
# reference : https://www.notion.so/skier-song9/Pytorch-9cfee0520ed6468a94b024ea35e48018?source=copy_link#1d2c8d3f60f58026b71ad399ead029a9    
scheduler_name: 'CosineAnnealingWarmupRestarts'
scheduler_params:
  T_max: 25 # ğŸ’¥ CosineAnnealingLRì˜ ê²½ìš°, epochs ê°’ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •. 
              # CosineAnnealingWarmupRestartsì˜ ê²½ìš° epochsì™€ ë™ì¼í•˜ê²Œ ë˜ëŠ” ì‘ê²Œ ì„¤ì •
  max_lr: 0.0001 # ìµœëŒ€ í•™ìŠµë¥ : 0.01 ~ 0.001ì‚¬ì´ì—ì„œ ì°¾ëŠ” ê²ƒì´ ì¼ë°˜ì 
  min_lr: 0.00001 # ìµœì†Œ í•™ìŠµë¥ : 1e-5 ~ 1e-6 ì‚¬ì´ì—ì„œ ì°¾ëŠ” ê²ƒì´ ì¼ë°˜ì 
  warmup: 0 # í•™ìŠµë¥ ì„ ì¬ì‹œì‘í•  ë•Œ min_lrì—ì„œ max_lrê¹Œì§€ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¬ epoch ìˆ˜ì´ë‹¤.
  gamma: 0.9 # í•™ìŠµë¥ ì„ ì¬ì‹œì‘í•  ë•Œ ì´ì „ max_lrì„ gammaë§Œí¼ ê°ì†Œì‹œì¼œ ì¬ì‹œì‘í•œë‹¤.

# Other variables
random_seed: 256
n_folds: 0 # number of folds for cross-validation
val_split_ratio: 0.15 # train-val split ë¹„ìœ¨
stratify: True # validation set ë¶„í•  ì‹œ stratify ì „ëµ ì‚¬ìš© ì—¬ë¶€
image_size: 384 # ë§Œì•½ multi-scale train/test ì‹œ Noneìœ¼ë¡œ ì„¤ì •

# Normalization
# full file tuning ì‹œ 0.5ê°€ ìœ ë¦¬
# pre-trained ëª¨ë¸ ì‚¬ìš© ì‹œ pre-trained ëª¨ë¸ì˜ mean, stdë¥¼ ì‚¬ìš©
norm_mean: [0.5, 0.5, 0.5]
norm_std: [0.5, 0.5, 0.5]

# Techniques
weighted_random_sampler: False
class_imbalance: 
  # aug_class: [1, 13, 14]
  # max_samples: 70
online_augmentation: True
online_aug: # Model Bì—ì„  mixup / cutmix ì¦ê°•ì„ í™œìš©
  mixup: True 
  cutmix: False
augmentation: # normal augmentation : dynamic augmentationì´ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì¼ë°˜ augmentationì€ ìë™ìœ¼ë¡œ ë¹„í™œì„±í™”ëœë‹¤.
  eda: True
  dilation: True
  erosion: True
  easiest: False
  stilleasy: False
  basic: False
  middle: False
  aggressive: False
# training epochì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì¦ê°•ê¸°ë²•ì„ ë³€í™˜í•˜ëŠ” ë°©ë²•
dynamic_augmentation:
  enabled: False
  policies:
    weak:
      end_epoch: 15
      augs: ['easiest', 'basic']
    middle:
      end_epoch: 35
      augs: ['stilleasy', 'eda']
    strong:
      end_epoch: 300
      augs: ['middle','aggressive']
  # you can add more augmentations here, e.g.,
  # random_crop: True
  # color_jitter: True
val_TTA: True # Validation ì‹œ, Test Time Augmentation ì‚¬ìš© ì—¬ë¶€
test_TTA: True # Inference ì‹œ, Test Time Augmentation ì‚¬ìš© ì—¬ë¶€
tta_dropout: False # inference ì‹œì—ë„ model.train() ëª¨ë“œë¥¼ ì‚¬ìš©í•´ dropoutì„ í™œì„±í™”í•˜ëŠ” ë°©ë²•
mixed_precision: True # Mixed Precision í•™ìŠµ ì‚¬ìš© ì—¬ë¶€ > ì‚¬ìš©í•˜ë©´ ë” í° batch_size í•™ìŠµ ê°€ëŠ¥

# Model hyperparameters
timm:
  activation: None # ReLU, LeakyReLU, ELU, SELU, GELU, Tanh, PReLU, SiLU
  drop_rate: 0.1
  drop_path_rate: 0.1
  # None ì…ë ¥ì‹œ timm ëª¨ë¸ì˜ ê¸°ë³¸ activation ì‚¬ìš©
  # dropoutê³¼ ê°™ì´ ì¶”ê°€ì ì¸ ì˜µì…˜ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°, key: value í˜•íƒœë¡œ ì „ë‹¬.
custom_layer: # custom classifier headë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ê²½ìš° ì„¤ì •. Noneì¼ ë•Œ custom head ì‚¬ìš© ì•ˆ í•¨. 
  # head_type: "simple_dropout"
  # drop: 0.1
  # activation: 'GELU'

# Training hyperparameters
epochs: 1000 # max epoch
patience: 7 # early stopping patience
batch_size: 10 # image_size, model_size, GPU RAM ì— ë”°ë¼ OOMì´ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì„¤ì •.

# W&B
wandb:
  project: "upstage-img-clf"
  log: True # log using wandb, if False then do not use wandb

# Paths
# test_data_dir: "/data/ephemeral/home/upstageailab-cv-classification-cv_5/data"
data_dir: "/data/ephemeral/home/upstageailab-cv-classification-cv_5/aug_data_500_new1"
train_data: train.csv