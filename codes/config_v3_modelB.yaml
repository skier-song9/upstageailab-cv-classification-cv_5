# Configuration for the training process
model_name: 'maxvit_base_tf_384.in21k_ft_in1k' # timm model name
pretrained: True # timm pretrained 가중치 사용 여부
fine_tuning: "full" # fine-tuning 방법론
  # full : pretrained=True, pretrained가중치를 전부 재학습시킨다. 
  # head : pretrained=True, model backbone 부분은 freeze하고 head 부분을 재학습시킨다.
  # custom : pretrained=True, backbone에서도 일부분을 재학습시킨다.
  # scratch : pretrained=False, 모델 구조만 사용하고 모든 가중치를 처음부터 학습시킨다.

# Loss Function
criterion: 'FocalLoss' # CrossEntropyLoss, FocalLoss, LabelSmoothingLoss
class_weighting: False # class에 가중치를 두어 Loss 계산
label_smooth: 0.0
# Optimizer
# optimizer name: SGD, RMSprop, Momentum, NAG, Adam, AdamW, NAdam, RAdam, Adafactor
# reference : https://www.notion.so/skier-song9/Pytorch-9cfee0520ed6468a94b024ea35e48018?source=copy_link#217c8d3f60f58044beeac55596433dc6
optimizer_name: 'Adafactor'
lr: 0.0001 # 1e-3 ~ 1e-4 사이의 값으로 시작
weight_decay: 0.00001 # 1e-2 ~ 1e-5 사이의 값이 일반적

# Scheduler
# scheduler_name : StepLR, ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmupRestarts
# reference : https://www.notion.so/skier-song9/Pytorch-9cfee0520ed6468a94b024ea35e48018?source=copy_link#1d2c8d3f60f58026b71ad399ead029a9    
scheduler_name: 'CosineAnnealingWarmupRestarts'
scheduler_params:
  T_max: 25 # 💥 CosineAnnealingLR의 경우, epochs 값과 동일하게 설정. 
              # CosineAnnealingWarmupRestarts의 경우 epochs와 동일하게 또는 작게 설정
  max_lr: 0.0001 # 최대 학습률: 0.01 ~ 0.001사이에서 찾는 것이 일반적
  min_lr: 0.00001 # 최소 학습률: 1e-5 ~ 1e-6 사이에서 찾는 것이 일반적
  warmup: 0 # 학습률을 재시작할 때 min_lr에서 max_lr까지 선형적으로 증가시킬 epoch 수이다.
  gamma: 0.9 # 학습률을 재시작할 때 이전 max_lr을 gamma만큼 감소시켜 재시작한다.

# Other variables
random_seed: 256
n_folds: 0 # number of folds for cross-validation
val_split_ratio: 0.15 # train-val split 비율
stratify: True # validation set 분할 시 stratify 전략 사용 여부
image_size: 384 # 만약 multi-scale train/test 시 None으로 설정

# Normalization
# full file tuning 시 0.5가 유리
# pre-trained 모델 사용 시 pre-trained 모델의 mean, std를 사용
norm_mean: [0.5, 0.5, 0.5]
norm_std: [0.5, 0.5, 0.5]

# Techniques
weighted_random_sampler: False
class_imbalance: 
  # aug_class: [1, 13, 14]
  # max_samples: 70
online_augmentation: True
online_aug: # Model B에선 mixup / cutmix 증강을 활용
  mixup: True 
  cutmix: False
augmentation: # normal augmentation : dynamic augmentation이 활성화되어 있으면 일반 augmentation은 자동으로 비활성화된다.
  eda: True
  dilation: True
  erosion: True
  easiest: False
  stilleasy: False
  basic: False
  middle: False
  aggressive: False
# training epoch에 따라 동적으로 증강기법을 변환하는 방법
dynamic_augmentation:
  enabled: False
  policies:
    weak:
      end_epoch: 15
      augs: ['easiest', 'basic']
    middle:
      end_epoch: 35
      augs: ['stilleasy', 'eda']
    strong:
      end_epoch: 300
      augs: ['middle','aggressive']
  # you can add more augmentations here, e.g.,
  # random_crop: True
  # color_jitter: True
val_TTA: True # Validation 시, Test Time Augmentation 사용 여부
test_TTA: True # Inference 시, Test Time Augmentation 사용 여부
tta_dropout: False # inference 시에도 model.train() 모드를 사용해 dropout을 활성화하는 방법
mixed_precision: True # Mixed Precision 학습 사용 여부 > 사용하면 더 큰 batch_size 학습 가능

# Model hyperparameters
timm:
  activation: None # ReLU, LeakyReLU, ELU, SELU, GELU, Tanh, PReLU, SiLU
  drop_rate: 0.1
  drop_path_rate: 0.1
  # None 입력시 timm 모델의 기본 activation 사용
  # dropout과 같이 추가적인 옵션이 존재하는 경우, key: value 형태로 전달.
custom_layer: # custom classifier head를 사용하고 싶은 경우 설정. None일 때 custom head 사용 안 함. 
  # head_type: "simple_dropout"
  # drop: 0.1
  # activation: 'GELU'

# Training hyperparameters
epochs: 1000 # max epoch
patience: 7 # early stopping patience
batch_size: 10 # image_size, model_size, GPU RAM 에 따라 OOM이 발생하지 않도록 설정.

# W&B
wandb:
  project: "upstage-img-clf"
  log: True # log using wandb, if False then do not use wandb

# Paths
# test_data_dir: "/data/ephemeral/home/upstageailab-cv-classification-cv_5/data"
data_dir: "/data/ephemeral/home/upstageailab-cv-classification-cv_5/aug_data_500_new1"
train_data: train.csv