import sys
import os
import yaml
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
import torch.nn.init as init
from torch.utils.data import Dataset, DataLoader, ConcatDataset
from torchvision import transforms
import timm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import wandb
from types import SimpleNamespace
import cv2
import albumentations as A
from albumentations.pytorch import ToTensorV2
from tqdm import tqdm
import copy
import time
from PIL import Image
from zoneinfo import ZoneInfo
from datetime import datetime
import argparse

#ğŸ“¢ project_root ì„¤ì • í•„ìˆ˜
project_root = '/data/ephemeral/home/upstageailab-cv-classification-cv_5'
sys.path.append(project_root)
from codes.gemini_utils import *
from codes.gemini_train import *
from codes.gemini_augmentation import *
from codes.gemini_evalute import *

if __name__ == "__main__":
    try:
        # python íŒŒì¼ ì‹¤í–‰í•  ë•Œ config.yaml íŒŒì¼ ì´ë¦„ì„ ì…ë ¥ë°›ì•„ì„œ ì„¤ì • íŒŒì¼ì„ ì§€ì •í•œë‹¤.
        parser = argparse.ArgumentParser(description="Run deep learning training with specified configuration.")
        parser.add_argument(
            '--config',
            type=str,
            default='config.yaml', # ê¸°ë³¸ê°’ ì„¤ì •
            help='Name of the configuration YAML file (e.g., config.yaml, experiment_A.yaml)'
        )
        
        args = parser.parse_args()

        # Yaml íŒŒì¼ ì½ê¸°
        cfg = load_config(
            config_path=os.path.join(os.path.dirname(os.path.abspath(__file__)), args.config)
        )
        # ëœë¤ì„± ì œì–´
        set_seed(cfg.random_seed)

        # device ì„¤ì •
        device = 'cpu'
        if torch.backends.mps.is_available():
            device = torch.device('mps')
        elif torch.cuda.is_available():
            device = torch.device('cuda')
        print("âš™ï¸ Device :",device)
        cfg.device = device
        CURRENT_TIME = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%y%m%d%H%M")
        print(f"âŒš ì‹¤í—˜ ì‹œê°„: {CURRENT_TIME}")


        # W&B ì„¤ì •
        # W&B ì‚¬ìš© ì•ˆ í•˜ë©´ run=None
        next_run_name = f"{CURRENT_TIME}-{cfg.model_name}-opt_{cfg.optimizer_name}-sch_{cfg.scheduler_name}-img{cfg.image_size}-{'on' if cfg.online_augmentation else 'off'}aug_{'_'.join([aug for aug, active in cfg.augmentation.items() if active])}-clsaug_{1 if cfg.class_imbalance else 0}-TTA_{1 if cfg.TTA else 0}-MP_{1 if cfg.mixed_precision else 0}"
        run = None 
        if hasattr(cfg, 'wandb') and cfg.wandb['log']:
            run = wandb.init(
                project=cfg.wandb['project'],
                name=next_run_name,
                config=vars(cfg),
            )

        ### submission í´ë” ìƒì„±
        # ëª¨ë¸ ì €ì¥, ì‹œê°í™” ê·¸ë˜í”„ ì €ì¥, submission íŒŒì¼ ë“±ë“± ì €ì¥ ìš©ë„
        submission_dir = os.path.join(cfg.data_dir, 'submissions', next_run_name)
        try:
            os.makedirs(submission_dir, exist_ok=False)
            # cfgì— ì¶”ê°€ 
            cfg.submission_dir = submission_dir
        except:
            raise ValueError("ê°™ì€ ì´ë¦„ì˜ submission í´ë”ê°€ ìˆìŠµë‹ˆë‹¤.", submission_dir)


        ### Data Load
        df = pd.read_csv(os.path.join(cfg.data_dir, "train.csv"))
        # Train-validation ë¶„í• 
        train_df, val_df = train_test_split(df, test_size=cfg.val_split_ratio, random_state=cfg.random_seed, stratify=df['target'] if cfg.stratify else None)

        # config.yamlì— class_imbalance ì„¤ì •í–ˆì„ ê²½ìš°,
        # offline cutout ì¦ê°•ìœ¼ë¡œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ë§ì¶˜ë‹¤.
        augmented_ids, augmented_labels = [], []
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ ì´ë¯¸ì§€ offline ì¦ê°•
        if hasattr(cfg, 'class_imbalance') and cfg.class_imbalance:
            augmented_ids, augmented_labels = augment_class_imbalance(cfg, train_df)
            imb_aug_df = pd.DataFrame({
                "ID": augmented_ids,
                "target": augmented_labels
            })
            # ê¸°ì¡´ train ë°ì´í„° í”„ë ˆì„ê³¼ ë³‘í•©
            train_df = pd.concat([train_df, imb_aug_df], ignore_index=True)
            train_df = train_df.reset_index(drop=True)

        
        ### Dataset & DataLoader 
        # Augmentation ì„¤ì •    
        train_transforms, val_transform, tta_transform = get_augmentation(cfg)

        if cfg.online_augmentation:
            train_dataset = ImageDataset(train_df, os.path.join(cfg.data_dir, "train"), transform=train_transforms[0])
        else:
            datasets = [ImageDataset(train_df, os.path.join(cfg.data_dir, "train"), transform=t) for t in train_transforms]
            train_dataset = ConcatDataset(datasets)

        val_dataset = ImageDataset(val_df, os.path.join(cfg.data_dir, "train"), transform=val_transform)

        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=4, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=4, pin_memory=True)

        # For TTA, we need a loader with raw images
        raw_transform = A.Compose([
            ToTensorV2()
        ])
        val_dataset_raw = ImageDataset(val_df, os.path.join(cfg.data_dir, "train"), transform=raw_transform)
        val_loader_raw = DataLoader(val_dataset_raw, batch_size=cfg.batch_size, shuffle=False, num_workers=4, pin_memory=True)

        ### Define TrainModule
        # Model
        model = get_timm_model(cfg)
        criterion = get_criterion(cfg)
        optimizer = get_optimizer(model, cfg)
        scheduler = get_scheduler(optimizer, cfg, steps_per_epoch=len(train_loader))

        trainer = TrainModule(
            model=model,
            criterion=criterion,
            optimizer=optimizer,
            scheduler=scheduler,
            train_loader=train_loader,
            valid_loader=val_loader,
            cfg=cfg,
            verbose=15,
            run=run
        )

        ### Train
        train_result = trainer.training_loop()
        if not train_result:
            raise ValueError("Failed to train model...")

        ### Save Model
        trainer.save_experiments(savepath=os.path.join(cfg.submission_dir, f'{next_run_name}.pht'))
        ## í•™ìŠµ ê²°ê³¼ ì‹œê°í™” ì €ì¥.
        trainer.plot_loss(
            show=False,
            savewandb=cfg.wandb['log'],
            savedir=cfg.submission_dir
        )


        ### Evaluate
        val_preds, val_f1 = do_validation(
            df=val_df, 
            model=trainer.model, 
            data=val_dataset_raw if cfg.TTA else val_loader, 
            transform_func=tta_transform, 
            cfg=cfg, 
            run=run, 
            show=False, 
            savepath=os.path.join(cfg.submission_dir, f"val_confusion_matrix{'_TTA' if cfg.TTA else ''}.png")
        )
        print("ğŸ“¢ Validation F1-score:",val_f1)


        # Inference
        test_df = pd.read_csv(os.path.join(cfg.data_dir, "sample_submission.csv"))

        if cfg.TTA:
            test_dataset_raw = ImageDataset(test_df, os.path.join(cfg.data_dir, "test"), transform=raw_transform)
            test_loader_raw = DataLoader(test_dataset_raw, batch_size=cfg.batch_size, shuffle=False, num_workers=4, pin_memory=True)
            print("Running TTA on test set...")
            test_preds = tta_predict(model, test_dataset_raw, tta_transform, device)
        else:
            test_dataset = ImageDataset(test_df, os.path.join(cfg.data_dir, "test"), transform=val_transform)
            test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=4, pin_memory=True)
            print("Running inference on test set...")
            test_preds = predict(model, test_loader, device)

        pred_df = pd.read_csv(os.path.join(cfg.data_dir, "sample_submission.csv"))
        pred_df['target'] = test_preds


        # Submission
        sample_submission_df = pd.read_csv(os.path.join(cfg.data_dir, "sample_submission.csv"))
        assert (sample_submission_df['ID'] == pred_df['ID']).all(), "pred_dfì—ì„œ test ì´ë¯¸ì§€ê°€ ì•„ë‹Œ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤."
        assert set(pred_df['target']).issubset(set(range(17))), "target ì»¬ëŸ¼ì— 0~16 ì™¸ì˜ ê°’ì´ ìˆìŠµë‹ˆë‹¤."

        submission_path = os.path.join(cfg.submission_dir, f"{next_run_name}.csv")
        pred_df.to_csv(submission_path, index=False)
        print(f"ğŸ“¢Submission file saved to {submission_path}")

        if run:
            # Log submission artifact
            artifact = wandb.Artifact(f'submission-{next_run_name}', type='submission')
            artifact.add_file(submission_path)
            run.log_artifact(artifact)
            run.finish()

    finally:
        if run:
            run.finish()
        if augmented_ids:
            ### Offline Augmentation íŒŒì¼ ì‚­ì œ
            delete_offline_augmented_images(cfg=cfg, augmented_ids=augmented_ids)